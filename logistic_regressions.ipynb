{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7485 labled tweets **after duplicate removals** \n",
    "+ *test.csv* - 3263 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 5988 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1497 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Simplier NLP Classifier Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ab98c-934b-4a3c-8a87-a97cce7aa76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50491f7f-8981-4e91-9eba-f7f42d4c9795",
   "metadata": {},
   "source": [
    "## Vectorize with all cleaned tweet tokens\n",
    "\n",
    "Since the max number of tokens in the cleaned original training data is 29 and 26 for the cleaned augmented data, a 30 token input will be selected.  This will give us an input to the model that is 30 (tokens / tweet) x (50 dimensions / token) = 1500 dimensions / tweet.\n",
    "\n",
    "Since all tweets will be less than 30 tokens, each input will be padded with the empty string token (<>).\n",
    "\n",
    "## Build feature matrices\n",
    "\n",
    "The following 4 feature matrices are built and exported so the can be read back in during modeling:\n",
    "\n",
    "+ **feats_matrix_aug.txt** - 7485 rows where each row is a vectorized tweet padded to 30 tokens where each token is represented by a 50d GloVe twitter embedding and the empty string is used as the padding token.  1500 cols are the tweets padded to 30 tokens which are each converted to a 50d GloVe embedding\n",
    "+ **feats_matrix_train_train.txt** - 80% of the original training data used to train each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_train_test.txt** - 20% of the original training data used to test each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_test.txt** - unlabeled test data provided by kaggle to test submissions, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "\n",
    "The first 3 feature matrices have the following corresponding labels (`feats_matrix_test.txt` are unlabeled tweets):\n",
    "+ labels_aug.txt\n",
    "+ labels_train_train.txt\n",
    "+ labels_train_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af07166-b4a7-4a1c-8d95-ef94f121a9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\llmamd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import projtools as pt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c34184-d8aa-44c6-be99-e0a2b99a99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train train feature matrix: (5988, 1500), shape of train train labels: (5988,)\n",
      "shape of train test feature matrix: (1497, 1500), shape of train test labels: (1497,)\n",
      "shape of augmented feature matrix: (7485, 1500), shape of augmented labels: (7485,)\n",
      "shape of (unlabeled) test feature matrix: (3263, 1500)\n"
     ]
    }
   ],
   "source": [
    "# read in the feature matrices\n",
    "feats_matrix_train_train = np.loadtxt('./data/feats_matrix_train_train.txt')\n",
    "feats_matrix_train_test = np.loadtxt('./data/feats_matrix_train_test.txt')\n",
    "feats_matrix_aug = np.loadtxt('./data/feats_matrix_aug.txt')\n",
    "feats_matrix_test = np.loadtxt('./data/feats_matrix_test.txt')\n",
    "# read in the labels\n",
    "labels_train_train = np.loadtxt('./data/labels_train_train.txt')\n",
    "labels_train_test = np.loadtxt('./data/labels_train_test.txt')\n",
    "labels_aug = np.loadtxt('./data/labels_aug.txt')\n",
    "print(f\"shape of train train feature matrix: {feats_matrix_train_train.shape}, shape of train train labels: {labels_train_train.shape}\")\n",
    "print(f\"shape of train test feature matrix: {feats_matrix_train_test.shape}, shape of train test labels: {labels_train_test.shape}\")\n",
    "print(f\"shape of augmented feature matrix: {feats_matrix_aug.shape}, shape of augmented labels: {labels_aug.shape}\")\n",
    "print(f\"shape of (unlabeled) test feature matrix: {feats_matrix_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90799600-c56a-4138-be3d-68b38b9ad4e3",
   "metadata": {},
   "source": [
    "## Logistic regression models\n",
    "\n",
    "### Just the original training data\n",
    "\n",
    "Intial results:**\n",
    "\n",
    "+ Train Training error:  0.201737\n",
    "+ Train Test error:  0.275217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98571b2-971b-43a7-9044-8d040d56597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "logreg_model_orig = SGDClassifier(loss=\"log_loss\", penalty=None)\n",
    "logreg_model_orig.fit(feats_matrix_train_train, labels_train_train)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = logreg_model_orig.coef_[0,:]\n",
    "b = logreg_model_orig.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac13ec8-668e-4c6a-bbf5-95fbe781fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.19505678022712092\n",
      "Test error:  0.2985971943887776\n"
     ]
    }
   ],
   "source": [
    "## Get predictions on training and test data\n",
    "preds_train = logreg_model_orig.predict(feats_matrix_train_train)\n",
    "preds_test = logreg_model_orig.predict(feats_matrix_train_test)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (labels_train_train > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (labels_train_test > 0.0))\n",
    "\n",
    "error_rate_train_train = float(errs_train)/len(labels_train_train)\n",
    "error_rate_train_test = float(errs_test)/len(labels_train_test)\n",
    "\n",
    "print(\"Training error: \", error_rate_train_train)\n",
    "print(\"Test error: \", error_rate_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9d5e1-824d-4dec-a018-9d6b02b1a10c",
   "metadata": {},
   "source": [
    "### The original training data PLUS the augmented data\n",
    "\n",
    "Now we'll add the augmented data to the training data and see if this improves our results.\n",
    "\n",
    "**Training error was cut in half, but test only improved slightly - initial results:**\n",
    "\n",
    "+ Training error:  0.108365\n",
    "+ Test error:  0.271209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7c7f049-faac-4431-b933-a72b4465f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13473, 1500) (13473,)\n"
     ]
    }
   ],
   "source": [
    "# create train + aug feature and label inputs\n",
    "train_train_aug_data = np.vstack((feats_matrix_train_train, feats_matrix_aug))\n",
    "train_train_aug_labels = np.hstack((labels_train_train, np.array(labels_aug)))\n",
    "print(train_train_aug_data.shape, train_train_aug_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a53722a-0020-47f2-8fdb-4b309c897fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "logreg_model_aug = SGDClassifier(loss=\"log_loss\", penalty=None)\n",
    "logreg_model_aug.fit(train_train_aug_data, train_train_aug_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = logreg_model_aug.coef_[0,:]\n",
    "b = logreg_model_aug.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc9a1f2-983c-49fb-86b3-5b9445770baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error with aug data: 0.10539597713946411\n",
      "Test error with aug data: 0.2758851035404142\n"
     ]
    }
   ],
   "source": [
    "## Get predictions on training and test data\n",
    "preds_train_aug = logreg_model_aug.predict(train_train_aug_data)\n",
    "preds_test_aug = logreg_model_aug.predict(feats_matrix_train_test)  # same train TEST partition, but different model\n",
    "\n",
    "## Compute errors\n",
    "errs_train_aug = np.sum((preds_train_aug > 0.0) != (train_train_aug_labels > 0.0))\n",
    "errs_test_aug = np.sum((preds_test_aug > 0.0) != (labels_train_test > 0.0))  # same train TEST partition, but different model\n",
    "\n",
    "error_rate_train_aug_train = float(errs_train_aug)/len(train_train_aug_labels)\n",
    "error_rate_train_aug_test = float(errs_test_aug)/len(labels_train_test)\n",
    "\n",
    "print(f\"Training error with aug data: {error_rate_train_aug_train}\")\n",
    "print(f\"Test error with aug data: {error_rate_train_aug_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c083d3-70c3-49a1-94cd-3ea154370ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Train error rate on original data: 0.1951, Train-Test error rate on original data: 0.2986\n",
      "Train-Train error rate on orig+aug data: 0.1054, Train-Test error rate on orig+aug data: 0.2759\n",
      "\n",
      "Train-Train accuracy on original data: 0.804943, Train-Train accuracy on original + aug data: 0.894604\n",
      "Train-Test accuracy on original data: 0.701403, Train-Test accuracy on original + aug data: 0.724115\n",
      "Train-Train accuracy change: 11.14%, Train-Test accuracy change: 3.24%\n"
     ]
    }
   ],
   "source": [
    "acc_train_train_orig = 1. - error_rate_train_train\n",
    "acc_train_train_aug = 1. - error_rate_train_aug_train\n",
    "acc_train_test_orig = 1. - error_rate_train_test\n",
    "acc_train_test_aug = 1. - error_rate_train_aug_test\n",
    "\n",
    "per_change_train_train_accuracy = 100. * ((acc_train_train_aug - acc_train_train_orig) / acc_train_train_orig)\n",
    "per_change_train_test_accuracy = 100. * ((acc_train_test_aug - acc_train_test_orig) / acc_train_test_orig)\n",
    "\n",
    "print(f\"Train-Train error rate on original data: {error_rate_train_train:.4f}, Train-Test error rate on original data: {error_rate_train_test:.4f}\")\n",
    "print(f\"Train-Train error rate on orig+aug data: {error_rate_train_aug_train:.4f}, Train-Test error rate on orig+aug data: {error_rate_train_aug_test:.4f}\")\n",
    "print()\n",
    "print(f\"Train-Train accuracy on original data: {acc_train_train_orig:.6f}, Train-Train accuracy on original + aug data: {acc_train_train_aug:.6}\")\n",
    "print(f\"Train-Test accuracy on original data: {acc_train_test_orig:.6f}, Train-Test accuracy on original + aug data: {acc_train_test_aug:.6f}\")\n",
    "print(f\"Train-Train accuracy change: {per_change_train_train_accuracy:.2f}%, Train-Test accuracy change: {per_change_train_test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4194e7-8e58-4108-8383-ffc05c995fc7",
   "metadata": {},
   "source": [
    "## How do the ROC curves compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8e75e-7826-4df7-b12c-c1070fa7c7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1f579-2c09-4564-b24c-d67c10480c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff714859-ddbd-410c-85fc-de047551abe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91b27-26e9-46d9-8079-689e9d238a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe11bb5-6846-46da-a716-eeea470742ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a5f6-db24-439f-a5c6-be4329d6e5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696941-a04d-4f3f-aeec-e133153a1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36480-1825-4f2a-8237-a9b1a73ff208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42422a-cfdf-4886-8076-ca74845a23f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025200d-3d63-402c-9545-7d93cbc76e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b72f0-3763-4933-962a-89048aa38d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7641e-4723-4cf3-b5a3-87377bc38c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
