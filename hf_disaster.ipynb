{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to explore whether a HuggingFace model (HFM) can enhance the performance of non-transformer-based text classification models by augmenting the training data.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files: *train.csv* (x labled tweets) and *test.csv* (y unlabled tweets)\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, x labeled tweets\n",
    "+ train_test.csv - not used to train model, used as *pseudo-test* data, y labeled tweets \n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 100 nodes in the hidden layer\n",
    "\n",
    "## HuggingFace Models\n",
    "\n",
    "The *TBD* Hugging Face transformer model was used to provide both uninformed and informed assistance through augmenting the data used to train the non-transformer-based models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "Two types of encodings are used to vectorize the inputs:\n",
    "\n",
    "1. One-hot encoding\n",
    "2. Twitter GloVe embedding: https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e8d7b-78a3-4581-9ae2-688cdf694678",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Manual inspection of train.csv\n",
    "\n",
    "The following issues observered in the data are listed below.  They are numbered to indicate the order in which they were fixed.  For example, spillover lines were fixed first, then lines that start with ??, etc.\n",
    "\n",
    "### 1. Spillover lines\n",
    "\n",
    "The first issue we see with this data is that while most of the samples are on there own line. Here are few examples:\n",
    "\n",
    ">`61,ablaze,,\"on the outside you're ablaze and alive`  \n",
    ">`but you're dead inside\",0`  \n",
    ">`74,ablaze,India,\"Man wife get six years jail for setting ablaze niece`  \n",
    ">`http://t.co/eV1ahOUCZA\",1`  \n",
    ">`86,ablaze,Inang Pamantasan,\"Progressive greetings!`  \n",
    ">  \n",
    ">`In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0`  \n",
    ">`117,accident,,\"mom: 'we didn't get home as fast as we wished'`  \n",
    ">`me: 'why is that?'`  \n",
    ">`mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0`\n",
    "\n",
    "The custom function `fix_spillover_lines` was written to fix these lines. Its code is available in the projtools module.\n",
    "\n",
    "### 2. TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e97b95-4b7e-4499-933a-5b3f5653c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562 3700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string as st\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To get around the \"\" error,\n",
    "# need specify encoding when reading this data in as described in the solution I upvoted here:\n",
    "# https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "# with open(\"./data/train.csv\", encoding=\"utf8\") as f:  # works, but setting errors removes unneeded chars\n",
    "with open(\"./data/train.csv\", encoding=\"utf8\", errors='ignore') as f_train:\n",
    "    content_train = f_train.readlines()\n",
    "\n",
    "with open(\"./data/test.csv\", encoding=\"utf8\", errors='ignore') as f_test:\n",
    "    content_test = f_test.readlines()\n",
    "\n",
    "print(len(content_train), len(content_test))  # 8562, 3700  BEFORE applying any fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7721fda8-fdfb-4584-b588-94398e0c45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "61,ablaze,,\"on the outside you're ablaze and alive\n",
      "but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece\n",
      "http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!\n",
      "\n",
      "In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n"
     ]
    }
   ],
   "source": [
    "# print some examples of spillover lines\n",
    "with open(\"./debug/train_debug_chunk.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    content_train_debug = f.readlines()\n",
    "\n",
    "for i in [0, 42, 43, 53, 54, 64, 65, 66]:\n",
    "    print(content_train_debug[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1f8deb-d216-4a62-bf8a-cd85e68e5295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "32,,,London is cool ;),0\n",
      "53,ablaze,\"London, UK\",On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N,0\n",
      "61,ablaze,,\"on the outside you're ablaze and alive but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!  In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n",
      "117,accident,,\"mom: 'we didn't get home as fast as we wished' me: 'why is that?' mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0\n",
      "119,accident,,Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident??,0\n",
      "120,accident,\"Arlington, TX\",#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k,1\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "# test the fix for the spillover lines on the training data\n",
    "fixed_train_debug = pt.fix_spillover_lines(content_train_debug)\n",
    "\n",
    "# for i, line in enumerate(fixed_list):\n",
    "#     print(i, line)\n",
    "\n",
    "# check that good lines are still good and spillover lines (*) are fixed\n",
    "#id = header 32  25 *61 *74 *86 *117 119 120\n",
    "for j in [0, 22, 36, 42, 52, 62, 81, 83, 84]:\n",
    "    print(fixed_train_debug[j])  # spillover lines are now consolidated to a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e3650a-b636-45c3-ab2d-719d5b0a64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the spillover lines in the train and test data, then write out fixed data\n",
    "fixed_train = pt.fix_spillover_lines(content_train)\n",
    "with open(file='./data/train_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_train_out:\n",
    "    for line in fixed_train:\n",
    "        f_train_out.write(line)\n",
    "        f_train_out.write('\\n')\n",
    "\n",
    "fixed_test = pt.fix_spillover_lines(content_test)\n",
    "with open(file='./data/test_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_test_out:\n",
    "    for line in fixed_test:\n",
    "        f_test_out.write(line)\n",
    "        f_test_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in training data that has fixed spillover lines\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('./data/train_clean_v01.csv', encoding=\"utf8\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
