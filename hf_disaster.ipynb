{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to explore whether a HuggingFace model (HFM) can enhance the performance of non-transformer-based text classification models by augmenting the training data.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7613 labled tweets\n",
    "+ *test.csv* - 3236 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 6090 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1523 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## HuggingFace Models\n",
    "\n",
    "The *TBD* Hugging Face transformer model was used to provide both uninformed and informed assistance through augmenting the data used to train the non-transformer-based models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "Two types of encodings are used to vectorize the inputs:\n",
    "\n",
    "1. One-hot encoding\n",
    "2. Twitter GloVe embedding: https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e8d7b-78a3-4581-9ae2-688cdf694678",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Manual inspection of train.csv\n",
    "\n",
    "The following issues observered in the data are listed below.  They are numbered to indicate the order in which they were processed.  For example, spillover lines were fixed first, then URLs, etc.  This order is important because removing things like punctuation too early would make things like identifying user names or hashtags in a tweet impossible or make URLs invalid.\n",
    "\n",
    "### 1. Spillover lines\n",
    "\n",
    "The first issue we see with this data is that while most of the samples are on there own line. Here are few examples:\n",
    "\n",
    ">`61,ablaze,,\"on the outside you're ablaze and alive`  \n",
    ">`but you're dead inside\",0`  \n",
    ">`74,ablaze,India,\"Man wife get six years jail for setting ablaze niece`  \n",
    ">`http://t.co/eV1ahOUCZA\",1`  \n",
    ">`86,ablaze,Inang Pamantasan,\"Progressive greetings!`  \n",
    ">  \n",
    ">`In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0`  \n",
    ">`117,accident,,\"mom: 'we didn't get home as fast as we wished'`  \n",
    ">`me: 'why is that?'`  \n",
    ">`mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0`\n",
    "\n",
    "The custom function `fix_spillover_lines` was written to fix these lines. Its code is available in the projtools module.\n",
    "\n",
    "### 2. Normalizing URLs\n",
    "\n",
    "Some tweet contain one or more URLs.  I assume that the content of a ULR does not contain any useful, but since a `<url>` token exists in the twitter gloVe embeddings, URLS will be replaced by this token.  \n",
    "\n",
    "Although the actual URL may not contain much useful information, the count of URLs occuring in a tweet may be a useful feature and are counted before they are normalized.  About 90% of the URLs in the training data are of the form `http://t.co/<10 digit hash>`. For example: `http://t.co/9FxPiXQuJt`.  In about 10% of cases, these URLs start with `https:\\\\`.\n",
    "\n",
    "#### 2.1 Counting URLs in each tweet\n",
    "\n",
    "The custom function `make_url_counts` is used to create a `url_count` feature/column.  This needs to be called before calling `replace_urls`.\n",
    "\n",
    "#### 2.2 Normalizing URLs\n",
    "\n",
    "The `replace_urls` function replaces each URL by the string \"<url>\" for the reasons stated above.  The needs to be called after `replace_urls` is called.\n",
    "\n",
    "\n",
    "### 3 Process Twitter-specifc characters\n",
    "\n",
    "Because the `@` and `#` characters have special meaning in tweets, they need to be processed before removing other punctuation.  When a `@<username>` is seen in a tweet, it is a reference to a user whose name is `username`.  When a `#<hashname>` is seen in a tweet, it specifies a hashtag which is a reference to all tweet tweets that use the `hashname` hashtag.  In processing these characters, `@<username>` is converted to `<user> username` and `#<hashname>` is converted to `<hashtag> hashname`.  These replacement tokens were selected because they also have mappings in the embeddings file described in the **Normalizing URLs** section.\n",
    "\n",
    "### 4. Tokenize text and clean up non-words\n",
    "\n",
    "#### 4.1 Contractions\n",
    "\n",
    "Contraction fragments are included as vectorized tokens in the twitter gloVe embeddings which means that we don't need to expand these manually.  The spaCy tokenizer will separate the first word from the contraction fragments: e.g \"you're\" will be tokenized into `[\"you\", \"'re\"]`.  Because the embeddings file has a listing for the contraction fragment token `'re` (as well as other contraction fragments such as 'm, 's, 'll, etc.), we don't need to convert these to their actual word forms (e.g. \"am\", \"is\", \"will\", etc.) before vectorizing.\n",
    "\n",
    "#### 4.2 Remove remaining punctuation\n",
    "\n",
    "The custom function `replace_with_space` is run to remove any remaining punctuation.\n",
    "\n",
    "#### 4.3 Remove digits\n",
    "\n",
    "The function `replace_numbers` is run to replace sets of consecutive digits digits with the token `<number>`.  As with the other token replacements, this one also was chosen because it has a mappling in the embeddings file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e97b95-4b7e-4499-933a-5b3f5653c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562 3700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string as st\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To get around the \"UnicodeDecodeError: 'charmap' codec can't decode byte ...\" error,\n",
    "# need specify encoding when reading this data in as described in the solution I upvoted here:\n",
    "# https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "# with open(\"./data/train.csv\", encoding=\"utf8\") as f:  # works, but setting errors removes unneeded chars\n",
    "with open(\"./data/train.csv\", encoding=\"utf8\", errors='ignore') as f_train:\n",
    "    content_train = f_train.readlines()\n",
    "\n",
    "with open(\"./data/test.csv\", encoding=\"utf8\", errors='ignore') as f_test:\n",
    "    content_test = f_test.readlines()\n",
    "\n",
    "print(len(content_train), len(content_test))  # 8562, 3700  BEFORE applying any fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7721fda8-fdfb-4584-b588-94398e0c45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "61,ablaze,,\"on the outside you're ablaze and alive\n",
      "but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece\n",
      "http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!\n",
      "\n",
      "In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n"
     ]
    }
   ],
   "source": [
    "# print some examples of spillover lines\n",
    "with open(\"./debug/train_debug_spillover_chunk.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    content_train_debug = f.readlines()\n",
    "\n",
    "for i in [0, 42, 43, 53, 54, 64, 65, 66]:\n",
    "    print(content_train_debug[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1f8deb-d216-4a62-bf8a-cd85e68e5295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "32,,,London is cool ;),0\n",
      "53,ablaze,\"London, UK\",On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N,0\n",
      "61,ablaze,,\"on the outside you're ablaze and alive but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!  In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n",
      "117,accident,,\"mom: 'we didn't get home as fast as we wished' me: 'why is that?' mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0\n",
      "119,accident,,Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident??,0\n",
      "120,accident,\"Arlington, TX\",#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k,1\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "# test the fix for the spillover lines on the training data\n",
    "fixed_train_debug = pt.fix_spillover_lines(content_train_debug)\n",
    "\n",
    "# for i, line in enumerate(fixed_list):\n",
    "#     print(i, line)\n",
    "\n",
    "# check that good lines are still good and spillover lines (*) are fixed\n",
    "#id = header 32  25 *61 *74 *86 *117 119 120\n",
    "for j in [0, 22, 36, 42, 52, 62, 81, 83, 84]:\n",
    "    print(fixed_train_debug[j])  # spillover lines are now consolidated to a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e3650a-b636-45c3-ab2d-719d5b0a64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the spillover lines in the train and test data, then write out fixed data\n",
    "# fixed_train = pt.fix_spillover_lines(content_train)\n",
    "# with open(file='./data/train_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_train_out:\n",
    "#     for line in fixed_train:\n",
    "#         f_train_out.write(line)\n",
    "#         f_train_out.write('\\n')\n",
    "\n",
    "# fixed_test = pt.fix_spillover_lines(content_test)\n",
    "# with open(file='./data/test_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_test_out:\n",
    "#     for line in fixed_test:\n",
    "#         f_test_out.write(line)\n",
    "#         f_test_out.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b01a52-b082-4ba3-a471-085251d987fe",
   "metadata": {},
   "source": [
    "## Normalizing URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48,ablaze,Birmingham,@bbcmtd Wholesale Markets ablaze <url>,1,1\n",
      "49,ablaze,Est. September 2012 - Bristol,We always try to bring the heavy. #metal #RT <url>,0,1\n",
      "50,ablaze,AFRICA,#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. <url>,1,1\n",
      "52,ablaze,\"Philadelphia, PA\",Crying out for more! Set me ablaze,0,0\n",
      "53,ablaze,\"London, UK\",On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE <url>,0,1\n",
      "54,ablaze,Pretoria,@PhDSquares #mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.,0,0\n",
      "55,ablaze,World Wide!!,INEC Office in Abia Set Ablaze - <url>,1,1\n",
      "56,ablaze,,Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  <url>,1,1\n",
      "57,ablaze,Paranaque City,Ablaze for you Lord :D,0,0\n",
      "59,ablaze,Live On Webcam,Check these out: <url> <url> <url> <url> #nsfw,0,4\n",
      "\n",
      "46,ablaze,London,Birmingham Wholesale Market is ablaze BBC News - Fire breaks out at Birmingham's Wholesale Market <url>,1\n",
      "47,ablaze,Niall's place | SAF 12 SQUAD |,@sunkxssedharry will you wear shorts for race ablaze ?,0\n",
      "51,ablaze,NIGERIA,#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriage crisis sets Nigerian Twitter ablaze... <url>,1\n",
      "58,ablaze,Live On Webcam,Check these out: <url> <url> <url> <url> #nsfw,4\n",
      "60,ablaze,\"Los Angeles, Califnordia\",\"PSA: IÛªm splitting my personalities.  ?? techies follow @ablaze_co ?? Burners follow @ablaze\",0\n",
      "69,ablaze,threeonefive. ,beware world ablaze sierra leone &amp; guap.,0\n",
      "70,ablaze,Washington State,Burning Man Ablaze! by Turban Diva <url> via @Etsy,1\n",
      "72,ablaze,\"Whoop Ass, Georgia\",Not a diss song. People will take 1 thing and run with it. Smh it's an eye opener though. He is about 2 set the game ablaze @CyhiThePrynce,0\n",
      "75,ablaze,India,Rape victim dies as she sets herself ablaze: A 16-year-old girl died of burn injuries as she set herself ablazeÛ_ <url>,1\n",
      "84,ablaze,,SETTING MYSELF ABLAZE <url>,1\n"
     ]
    }
   ],
   "source": [
    "# normalize urls\n",
    "with open(\"./data/train_clean_v01.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    v01_train_lines = f.readlines()\n",
    "\n",
    "with open(\"./data/test_clean_v01.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    v01_test_lines = f.readlines()\n",
    "\n",
    "# first, count the urls before replacing them\n",
    "v02_train_lines = pt.make_url_counts(v01_train_lines)\n",
    "v02_test_lines = pt.make_url_counts(v01_test_lines)\n",
    "\n",
    "# second, replace the urls with the <url> token\n",
    "v02_train_lines = pt.replace_urls(v02_train_lines)\n",
    "v02_test_lines = pt.replace_urls(v02_test_lines)\n",
    "\n",
    "\n",
    "# look at some lines that have urls to check if they are getting counted\n",
    "for i in range(32, 42):\n",
    "    print(v02_train_lines[i])\n",
    "print()\n",
    "for j in range(16, 26):\n",
    "    print(v02_test_lines[j])\n",
    "\n",
    "# write file with url fixes\n",
    "# pt.write_lines_to_csv(v02_train_lines, \"./data/train_clean_v02.csv\")\n",
    "# pt.write_lines_to_csv(v02_test_lines, \"./data/test_clean_v02.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42d3de-441d-4519-955d-427b91937e5b",
   "metadata": {},
   "source": [
    "## Process Twitter-specifc characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3fd77a-93f5-4cfc-8943-0e9d12faf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process twitter-specific chars\n",
    "with open(\"./data/train_clean_v02.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    v02_train_lines = f.readlines()\n",
    "\n",
    "with open(\"./data/test_clean_v02.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    v02_test_lines = f.readlines()\n",
    "\n",
    "v03_train_lines = pt.replace_twitter_specials(v02_train_lines)\n",
    "v03_test_lines = pt.replace_twitter_specials(v02_test_lines)\n",
    "\n",
    "# write file with twitter-specific chars fixes\n",
    "# pt.write_lines_to_csv(v03_train_lines, \"./data/train_clean_v03.csv\")\n",
    "# pt.write_lines_to_csv(v03_test_lines, \"./data/test_clean_v03.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bf30b-d7f0-4f4a-a59d-a9b2c4d5fdae",
   "metadata": {},
   "source": [
    "## Tokenize text and clean up non-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e35161-05ba-4d93-8622-703758032af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 174 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load gloVe embeddings into dict\n",
    "dict_embeddings = pt.get_glove_embeds(embed_path = \"./embeddings/glove.twitter.200d.TEST.txt\")\n",
    "# test the embeddings read\n",
    "test_keys = [\"<user>\", \"na\", \"all\"]\n",
    "# for key in test_keys:\n",
    "#     print(\"key = \", key)\n",
    "#     print(\"value = \", dict_embeddings[key])\n",
    "\n",
    "# load embeddings into spaCy Vocab\n",
    "\n",
    "\n",
    "# export Vocab to save time later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd0fa146-c11f-43d2-b72e-f1518f856466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy en_core_web_lg model loaded...\n",
      "loading token  0  which is  <user>\n",
      "loading token  10  which is  i\n",
      "loading token  20  which is  )\n",
      "loading token  30  which is  no\n",
      "loading token  40  which is  la\n",
      "loading token  50  which is  o\n",
      "loading token  60  which is  >\n",
      "loading token  70  which is  are\n",
      "loading token  80  which is  we\n",
      "loading token  90  which is  ♥\n",
      "loading token  100  which is  _\n",
      "loading token  110  which is  now\n",
      "loading token  120  which is  ~\n",
      "loading token  130  which is  people\n",
      "loading token  140  which is  're\n",
      "loading token  150  which is  >>\n",
      "loading token  160  which is  [\n",
      "loading token  170  which is  q\n",
      "token <user> has vector:\n",
      "[ 3.1553e-01  5.3765e-01  1.0177e-01  3.2553e-02  3.7980e-03  1.5364e-02\n",
      " -2.0344e-01  3.3294e-01 -2.0886e-01  1.0061e-01  3.0976e-01  5.0015e-01\n",
      "  3.2018e-01  1.3537e-01  8.7039e-03  1.9110e-01  2.4668e-01 -6.0752e-02\n",
      " -4.3623e-01  1.9302e-02  5.9972e-01  1.3444e-01  1.2801e-02 -5.4052e-01\n",
      "  2.7387e-01 -1.1820e+00 -2.7677e-01  1.1279e-01  4.6596e-01 -9.0685e-02\n",
      "  2.4253e-01  1.5654e-01 -2.3618e-01  5.7694e-01  1.7563e-01 -1.9690e-02\n",
      "  1.8295e-02  3.7569e-01 -4.1984e-01  2.2613e-01 -2.0438e-01 -7.6249e-02\n",
      "  4.0356e-01  6.1582e-01 -1.0064e-01  2.3318e-01  2.2808e-01  3.4576e-01\n",
      " -1.4627e-01 -1.9880e-01  3.3232e-02 -8.4885e-01 -2.5684e-01  2.6369e-01\n",
      "  2.9562e-01  1.8470e-01 -2.0668e-01 -1.3297e-02  1.2233e-01 -4.7751e-01\n",
      " -1.7202e-01 -1.4577e-01  4.7446e-02 -1.5824e-01  5.4215e-02 -1.9426e-01\n",
      " -8.1484e-02  9.9009e-02  1.0159e-01  4.3571e-02  5.0245e-01  1.3362e-01\n",
      "  6.5985e-02  3.2969e-02 -2.0170e-01 -5.6905e-01 -1.3203e-01  7.3347e-02\n",
      " -6.3728e-02 -2.7960e-01 -3.8481e-01 -2.0193e-02  2.2298e-01 -5.9115e-02\n",
      "  4.5198e-02 -1.3995e-01 -1.3299e-01  4.7309e-01 -2.1874e-02  3.8758e-01\n",
      " -7.4926e-02 -2.8093e-03 -2.9829e-01 -7.4987e-02 -5.8542e-01 -1.8065e-01\n",
      " -4.1805e-02  4.1938e-01  4.1004e-01 -5.9110e-01  1.0459e-01  1.0724e-01\n",
      "  6.9768e-01 -1.5901e-01 -5.9596e-02  2.9368e-01 -1.9609e-01  3.9124e-01\n",
      " -2.9333e-01 -5.0833e-03 -3.7854e-01  3.3858e-01  2.4782e-01  2.9144e-01\n",
      " -2.2833e-01  1.9421e-01 -5.5409e-02  1.0322e-01  3.8963e-01 -2.7813e-01\n",
      "  2.1963e-01  4.0014e-01  7.1036e-02 -7.9786e-02  1.9534e-01 -6.9432e-01\n",
      " -9.3075e-02 -1.3729e-01 -5.4014e-01  5.7165e-01  2.4443e-01  3.6741e-02\n",
      "  3.3606e-02 -1.4398e-01  2.5873e-01  8.9008e-02  1.1109e-01  3.8387e-01\n",
      "  1.9013e-01 -2.3252e-01 -2.6271e-01 -2.6936e-01 -3.2409e-01  5.5236e-01\n",
      " -4.6158e-01 -1.1086e-01 -3.8417e-01  5.9605e-01 -1.4479e-01 -2.8762e-01\n",
      " -2.9638e-01  2.1889e-01 -4.1257e+00  6.9382e-01 -2.6307e-01 -1.3691e-02\n",
      "  3.2916e-02  1.7627e-02 -2.9090e-02  4.2807e-01 -2.0815e-01  5.0598e-01\n",
      " -8.0836e-02  4.5083e-01 -1.1466e-01 -2.7782e-01 -3.8373e-02  1.5672e-01\n",
      " -1.0899e-02  8.2053e-02 -1.9766e-01  2.0574e-01 -7.5329e-02  8.3754e-02\n",
      " -7.2767e-01  1.0403e-01  4.2831e-01  4.1023e-01 -9.7314e-02  1.5128e-01\n",
      "  3.9287e-01 -1.7807e-01 -2.9196e-02  5.7502e-01 -1.7823e-01 -7.6488e-01\n",
      " -4.7383e-01 -2.1984e-01  2.1190e-01 -1.5729e-02 -6.2927e-02  2.6674e-01\n",
      " -2.3617e-01  1.8109e-01 -2.6583e-01  9.0904e-02 -8.1205e-01 -4.5664e-01\n",
      " -4.6540e-01  5.2066e-01]\n",
      "token na has vector:\n",
      "[ 5.5666e-01 -1.3468e-01  2.4027e-01 -5.8919e-02 -3.2611e-01  6.1445e-02\n",
      "  1.1206e-01  2.8578e-01  2.7213e-01 -2.3985e-02 -4.5048e-01 -1.1851e-01\n",
      "  1.5999e-01  5.0799e-01 -3.0718e-01  6.9398e-02 -2.3729e-01  3.0359e-02\n",
      "  1.9681e-01 -6.5963e-01  5.4523e-03  1.0057e-01 -1.8616e-01 -3.0902e-01\n",
      "  5.1938e-01 -2.4094e+00 -5.2066e-01 -2.6670e-01  3.8333e-02 -1.6018e-01\n",
      "  7.2337e-01 -4.9379e-01 -3.8244e-01 -7.1738e-01 -3.6330e-01  9.1785e-02\n",
      "  3.0681e-01  7.5470e-02 -1.6946e+00  4.7912e-01 -1.5856e+00  5.1544e-01\n",
      "  2.9480e-01 -2.1203e-03 -2.2300e-01  1.4338e-01 -7.9541e-01 -2.8305e-02\n",
      " -8.2238e-01  2.0822e-01  3.5483e-01  3.5355e-01  4.3073e-01  5.0024e-01\n",
      "  2.6324e-01 -7.1436e-01 -2.8736e-01 -2.0542e-02 -2.5319e-01  8.6959e-02\n",
      " -4.5752e-02  5.7971e-01 -2.5681e-01 -1.6447e-01 -1.2729e+00  4.8321e-01\n",
      "  1.2566e-01 -1.0851e+00 -4.3285e-01 -1.4404e-01  9.8801e-01 -7.6991e-01\n",
      " -2.2267e-01 -4.5103e-02 -8.6885e-02 -9.8926e-02 -2.3213e-01 -2.0145e-01\n",
      "  7.6661e-01  2.8358e-01 -1.3562e+00 -1.2340e-01  1.0001e-01 -5.6422e-01\n",
      "  7.7272e-02  1.3412e-01  6.4828e-01 -3.8389e-01 -8.8672e-01 -3.5955e-01\n",
      "  8.0589e-01 -5.7934e-01 -1.2869e-01  8.4400e-02 -5.8450e-01 -8.1918e-01\n",
      "  6.5740e-01  2.5920e-01  4.1553e-01  8.0770e-02 -3.2170e-01  4.8051e-01\n",
      "  6.0548e-01 -6.1126e-02  7.3960e-02 -2.5322e-01  6.0538e-01 -5.1633e-01\n",
      "  4.2848e-01  1.7887e-01  1.2631e-02  1.5078e-01  1.4560e-01 -2.8393e-01\n",
      " -5.3408e-01 -1.1126e-01  3.4048e-01 -1.9116e-01 -3.3427e-02  4.7202e-01\n",
      "  2.0127e-02 -1.9564e-01  3.6701e-01 -4.6361e-01  7.1929e-03  1.1889e-01\n",
      " -3.7653e-01 -5.0224e-02 -7.3642e-01 -7.7379e-02 -2.3233e-01 -6.6539e-01\n",
      " -1.5317e-01  5.8392e-01 -6.2735e-01 -6.4971e-01 -3.3374e-01  7.0166e-01\n",
      " -3.7085e-02  5.5028e-01  2.3460e-01  4.6627e-01 -1.2746e-01 -1.2865e-01\n",
      " -4.1018e-01  1.2503e+00  5.0172e-01  8.9314e-01  1.2875e-01  3.1324e-01\n",
      "  1.2650e+00 -6.8043e-01 -4.3897e+00 -4.8910e-01 -2.8001e-01 -3.2815e-01\n",
      "  1.5985e-01  3.3590e-01 -5.4306e-02  1.5380e-01  4.7833e-01  2.3870e-02\n",
      " -1.4844e-02 -1.5998e-01 -1.1367e-02 -6.3943e-01  6.2936e-01 -4.6098e-01\n",
      " -2.7917e-01  4.5798e-01 -4.7891e-02  3.3599e-01 -9.9564e-02 -5.7525e-01\n",
      "  8.7354e-03 -9.5651e-01 -5.4245e-01  6.0714e-01  9.4844e-02  4.4668e-01\n",
      " -8.3420e-01 -7.1821e-02  2.4449e-01  3.3912e-01  1.8016e-01  8.0875e-01\n",
      " -7.7083e-01 -6.0950e-02  4.4641e-01  5.5561e-01  2.0857e-02  7.0480e-01\n",
      " -3.4434e-01 -5.6157e-01 -9.7712e-01  1.7981e-02  8.4308e-02  2.5780e-01\n",
      "  7.7217e-01  7.9560e-02]\n",
      "token all has vector:\n",
      "[ 2.6406e-01  1.7203e-01  8.2823e-02  2.6612e-01  9.1462e-02  9.8264e-02\n",
      "  2.0179e-01 -3.4539e-01 -2.6226e-01  1.7898e-01 -1.5378e-01 -2.2304e-01\n",
      " -1.2558e+00 -2.4566e-01 -2.6994e-01 -3.9058e-01 -3.0531e-01 -5.2871e-01\n",
      " -2.6624e-01 -1.6240e-01 -3.5035e-02 -3.4628e-01  3.4868e-01 -2.0686e-01\n",
      "  2.8040e-01  1.2050e+00  3.7025e-01 -3.6895e-01 -4.1991e-01  1.2469e-01\n",
      " -5.5385e-01 -2.4447e-01  1.0144e-01  9.1370e-01  1.9434e-02  2.4264e-01\n",
      " -2.2058e-01  2.8641e-01 -2.9471e-01  2.1241e-02  3.9811e-02 -4.3060e-01\n",
      "  3.1291e-01 -1.9264e-01 -6.3141e-02 -1.3230e-01  1.0036e-01 -3.7520e-01\n",
      " -3.6421e-01 -3.9440e-01 -9.7711e-02  2.7564e-03  2.5729e-01 -6.6237e-02\n",
      " -8.7918e-03  1.4739e-01 -1.0593e+00 -4.8599e-02  1.6293e-01 -6.2637e-02\n",
      "  5.3444e-03  1.4912e-01  3.4434e-02  3.9821e-01  2.4560e-01 -4.0136e-01\n",
      " -4.9206e-02 -2.0548e-01  1.5181e-01  9.8514e-02 -1.3732e-01  5.3009e-02\n",
      " -1.3365e-01  7.6907e-02  5.4490e-01  5.1054e-01  2.3874e-02 -6.1097e-01\n",
      " -2.0704e-01 -1.7839e-01  7.0654e-01  1.2419e-01  3.9198e-01  1.8957e-01\n",
      "  1.7838e-02  3.2653e-01 -6.4342e-02 -4.6905e-01  2.3685e-01  4.2143e-04\n",
      "  1.2431e-01 -1.7836e-01 -1.0863e-02  1.6566e-01  4.6264e-02 -3.0568e-01\n",
      "  5.8729e-03 -1.6221e-01 -1.6222e-02  4.7594e-02 -7.5343e-03 -2.7363e-02\n",
      "  3.5010e-02 -2.7982e-01  2.0808e-01  1.3404e-01  1.3099e-01 -3.2748e-02\n",
      " -2.2445e-01 -1.2125e-01  1.1488e-01  5.1940e-01 -2.1801e-01  1.2055e-01\n",
      "  4.0006e-02  5.4451e-02  4.8876e-02 -6.0423e-01  1.9556e-01  1.8667e-01\n",
      " -4.2187e-02 -1.6626e-01 -2.3186e-01 -1.6305e-01  4.6566e-01 -3.3784e-02\n",
      "  7.6045e-02 -3.3703e-02  6.0591e-02 -1.6843e-01  1.6161e-01 -4.6522e-02\n",
      " -1.0978e-01  7.0006e-02  2.0004e-01  2.5302e-01  2.7873e-01  5.2688e-03\n",
      "  1.9027e-01 -1.6284e-01 -1.1194e-01  2.1453e-01  2.2420e-01 -6.7911e-02\n",
      " -3.9131e-01 -2.9751e-01  4.7195e-02  8.6759e-03  4.6365e-01 -1.5167e-01\n",
      "  1.8536e-02  5.4117e-01 -6.1197e+00 -1.7789e-01  4.0557e-01  7.2398e-02\n",
      " -1.3880e-01  5.7984e-02 -9.9897e-01  7.2710e-01  2.9353e-01  1.9088e-01\n",
      " -3.5127e-01 -9.3896e-02  6.5531e-02  4.9532e-01  5.1054e-02  1.6269e-01\n",
      "  2.5056e-01  3.3144e-01 -1.6183e-01 -1.3843e-02  1.0459e-01 -5.6778e-02\n",
      "  2.3648e-01  1.9150e-02  2.5318e-03  1.0432e-01 -2.0739e-01  9.3981e-02\n",
      "  1.1655e-01 -4.0880e-01 -1.5513e-01 -8.1173e-03 -4.4464e-01 -5.3586e-02\n",
      "  2.6280e-01  2.4804e-02 -7.5572e-01 -2.2838e-02 -4.1089e-01  1.5731e-01\n",
      "  4.9716e-01 -6.2489e-02  1.1969e-01  3.8380e-01 -7.8333e-02  3.1139e-01\n",
      " -8.8639e-02 -8.1436e-02]\n"
     ]
    }
   ],
   "source": [
    "import spacy as sp\n",
    "\n",
    "nlp = sp.load(\"en_core_web_md\")  # load language model\n",
    "print(\"spaCy en_core_web_lg model loaded...\")\n",
    "\n",
    "# load embeddings into spaCy Vocab\n",
    "vocab = sp.vocab.Vocab()\n",
    "for i, (token, vector) in enumerate(dict_embeddings.items()):\n",
    "    if i % 10 == 0:\n",
    "        print(\"loading token \", i, \" which is \", token)\n",
    "    vocab.set_vector(token, vector)\n",
    "\n",
    "# test the vocab load - grab a few vectors\n",
    "for token in test_keys:\n",
    "    print(f\"token {token} has vector:\")\n",
    "    print(vocab.get_vector(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1168137-c763-48b5-82b4-d2858431e4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19267b-17c9-4989-a4d8-1a394f80fcf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b72f0-3763-4933-962a-89048aa38d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "\n",
    "# break out the text column so it can be operated on separately\n",
    "df_train04 = pd.read_csv('./data/train_clean_v03.csv', encoding=\"utf8\")\n",
    "df_test04 = pd.read_csv('./data/test_clean_v03.csv', encoding=\"utf8\")\n",
    "\n",
    "# df_train04 = pd.read_csv('./data/train_clean_v01.csv', encoding=\"utf8\")\n",
    "# df_test04 = pd.read_csv('./data/test_clean_v01.csv', encoding=\"utf8\")\n",
    "\n",
    "train_text_lines = df_train04['text'].to_list()\n",
    "test_text_lines = df_test04['text'].to_list()\n",
    "\n",
    "# print(\"..._clean_v03.csv tweets text loaded...\")\n",
    "print(\"..._clean_v01.csv tweets text loaded...\")\n",
    "\n",
    "nlp = sp.load(\"en_core_web_lg\")  # load language model\n",
    "print(\"spaCy en_core_web_lg model loaded...\")\n",
    "\n",
    "train_docs = []\n",
    "# create spacy doc objects\n",
    "for train_line in train_text_lines:\n",
    "    train_docs.append(nlp(train_line))\n",
    "\n",
    "print\n",
    "\n",
    "test_docs = []\n",
    "for test_line in test_text_lines:\n",
    "    test_docs.append(nlp(test_line))\n",
    "\n",
    "for i in range(0, 40):\n",
    "    print([token.text for token in train_docs[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need understand available tokens in lg spaCy model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data as dataframe\n",
    "# import pandas as pd\n",
    "\n",
    "# df_train = pd.read_csv('./data/train_clean_v01.csv', encoding=\"utf8\")\n",
    "# print(df_train.shape)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
