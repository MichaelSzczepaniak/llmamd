{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to explore whether a HuggingFace model (HFM) can enhance the performance of non-transformer-based text classification models by augmenting the training data.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files: *train.csv* (x labled tweets) and *test.csv* (y unlabled tweets)\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, x labeled tweets\n",
    "+ train_test.csv - not used to train model, used as *pseudo-test* data, y labeled tweets \n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 100 nodes in the hidden layer\n",
    "\n",
    "## HuggingFace Models\n",
    "\n",
    "The *TBD* Hugging Face transformer model was used to provide both uninformed and informed assistance through augmenting the data used to train the non-transformer-based models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "Two types of encodings are used to vectorize the inputs:\n",
    "\n",
    "1. One-hot encoding\n",
    "2. Twitter GloVe embedding: https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e8d7b-78a3-4581-9ae2-688cdf694678",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Manual inspection of train.csv\n",
    "\n",
    "The following issues observered in the data are listed below.  They are numbered to indicate the order in which they were processed.  For example, spillover lines were fixed first, then URLs, etc.  This order is important because removing things like punctuation too early would make things like identifying user names or hashtags in a tweet impossible or make URLs invalid.\n",
    "\n",
    "### 1. Spillover lines\n",
    "\n",
    "The first issue we see with this data is that while most of the samples are on there own line. Here are few examples:\n",
    "\n",
    ">`61,ablaze,,\"on the outside you're ablaze and alive`  \n",
    ">`but you're dead inside\",0`  \n",
    ">`74,ablaze,India,\"Man wife get six years jail for setting ablaze niece`  \n",
    ">`http://t.co/eV1ahOUCZA\",1`  \n",
    ">`86,ablaze,Inang Pamantasan,\"Progressive greetings!`  \n",
    ">  \n",
    ">`In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0`  \n",
    ">`117,accident,,\"mom: 'we didn't get home as fast as we wished'`  \n",
    ">`me: 'why is that?'`  \n",
    ">`mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0`\n",
    "\n",
    "The custom function `fix_spillover_lines` was written to fix these lines. Its code is available in the projtools module.\n",
    "\n",
    "### 2. Normalizing URLs\n",
    "\n",
    "Some tweet contain one or more URLs.  I assume that the content of a ULR does not contain any useful.  However, the count of URLs occuring in a tweet may be a useful feature and are counted before removing them.  About 90% of the URLs in the training data are of the form `http://t.co/<10 digit hash>`. For example: `http://t.co/9FxPiXQuJt`.  In about 10% of cases, these URLs start with `https:\\\\`.\n",
    "\n",
    "#### 2.1 Counting URLs in each tweet\n",
    "\n",
    "The custom function `make_url_counts` is used to create a `url_count` feature/column.  This is called before removing the URLs as described in the next section.\n",
    "\n",
    "#### 2.2 Removing URLs\n",
    "\n",
    "The `replace_urls` function replaces each URL by the string \"web link\".\n",
    "\n",
    "### 3. Expanding contractions\n",
    "\n",
    "The last step before building a text classification model is the encoding of words into vectors.  Before we do this encoding, each text entry needs to consist of single words.  Contractions are especially common in the text of tweets as they allow the communicator to relay the same amount of information with less characters.  But because contractions like \"I'm\", \"you're\", etc. represent combinations of words like \"I am\" and \"you are\" respectively, they need to be expanded into their corresponding single-word forms before they are encoded.\n",
    "\n",
    "This expansion needs to happen before removing punctutation since contractions are typically created with an apostrophy (`'`).  The `contractions` library was chosen to expand contractions.\n",
    "\n",
    "### 4. Removing punctuation and digits\n",
    "\n",
    "Removing punctuation and digits needs to be done just on the text of the tweets which are in the `text` column.  Each column will be extracted by reading the `csv` file into a dataframe so the `text` column can be worked on in isolation.\n",
    "\n",
    "#### 4.1 Process Twitter-specifc characters\n",
    "\n",
    "Because the `@` and `#` characters have special meaning in tweets, they need to be processed before removing other punctuation.  When a `@<username>` is seen in a tweet, it is a reference to a user whose name is `username`.  When a `#<hashname>` is seen in a tweet, it specifies a hashtag which is a reference to all tweet tweets that use the `hashname` hashtag.  In processing these characters, `@<username>` is converted to `at username` and `#<hashname>` is converted to `hashtag hashname`.\n",
    "\n",
    "#### 4.2 Remove remaining punctuation\n",
    "\n",
    "The custom function `replace_with_space` is run to remove any remaining punctuation.\n",
    "\n",
    "#### 4.3 Remove digits\n",
    "\n",
    "The function `replace_with_space` is run to remove any remaining digits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e97b95-4b7e-4499-933a-5b3f5653c938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8562 3700\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import string as st\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# To get around the \"UnicodeDecodeError: 'charmap' codec can't decode byte ...\" error,\n",
    "# need specify encoding when reading this data in as described in the solution I upvoted here:\n",
    "# https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "# with open(\"./data/train.csv\", encoding=\"utf8\") as f:  # works, but setting errors removes unneeded chars\n",
    "with open(\"./data/train.csv\", encoding=\"utf8\", errors='ignore') as f_train:\n",
    "    content_train = f_train.readlines()\n",
    "\n",
    "with open(\"./data/test.csv\", encoding=\"utf8\", errors='ignore') as f_test:\n",
    "    content_test = f_test.readlines()\n",
    "\n",
    "print(len(content_train), len(content_test))  # 8562, 3700  BEFORE applying any fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7721fda8-fdfb-4584-b588-94398e0c45c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "61,ablaze,,\"on the outside you're ablaze and alive\n",
      "but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece\n",
      "http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!\n",
      "\n",
      "In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n"
     ]
    }
   ],
   "source": [
    "# print some examples of spillover lines\n",
    "with open(\"./debug/train_debug_chunk.txt\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    content_train_debug = f.readlines()\n",
    "\n",
    "for i in [0, 42, 43, 53, 54, 64, 65, 66]:\n",
    "    print(content_train_debug[i].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1f8deb-d216-4a62-bf8a-cd85e68e5295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target\n",
      "32,,,London is cool ;),0\n",
      "53,ablaze,\"London, UK\",On plus side LOOK AT THE SKY LAST NIGHT IT WAS ABLAZE http://t.co/qqsmshaJ3N,0\n",
      "61,ablaze,,\"on the outside you're ablaze and alive but you're dead inside\",0\n",
      "74,ablaze,India,\"Man wife get six years jail for setting ablaze niece http://t.co/eV1ahOUCZA\",1\n",
      "86,ablaze,Inang Pamantasan,\"Progressive greetings!  In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0\n",
      "117,accident,,\"mom: 'we didn't get home as fast as we wished' me: 'why is that?' mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0\n",
      "119,accident,,Can wait to see how pissed Donnie is when I tell him I was in ANOTHER accident??,0\n",
      "120,accident,\"Arlington, TX\",#TruckCrash Overturns On #FortWorth Interstate http://t.co/Rs22LJ4qFp Click here if you've been in a crash&gt;http://t.co/Ld0unIYw4k,1\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "# test the fix for the spillover lines on the training data\n",
    "fixed_train_debug = pt.fix_spillover_lines(content_train_debug)\n",
    "\n",
    "# for i, line in enumerate(fixed_list):\n",
    "#     print(i, line)\n",
    "\n",
    "# check that good lines are still good and spillover lines (*) are fixed\n",
    "#id = header 32  25 *61 *74 *86 *117 119 120\n",
    "for j in [0, 22, 36, 42, 52, 62, 81, 83, 84]:\n",
    "    print(fixed_train_debug[j])  # spillover lines are now consolidated to a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79e3650a-b636-45c3-ab2d-719d5b0a64a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the spillover lines in the train and test data, then write out fixed data\n",
    "# fixed_train = pt.fix_spillover_lines(content_train)\n",
    "# with open(file='./data/train_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_train_out:\n",
    "#     for line in fixed_train:\n",
    "#         f_train_out.write(line)\n",
    "#         f_train_out.write('\\n')\n",
    "\n",
    "# fixed_test = pt.fix_spillover_lines(content_test)\n",
    "# with open(file='./data/test_clean_v01.csv', mode='w', encoding=\"utf8\", errors='ignore') as f_test_out:\n",
    "#     for line in fixed_test:\n",
    "#         f_test_out.write(line)\n",
    "#         f_test_out.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# count urls and add url_count column\n",
    "# train_url_counts = pt.make_url_counts(fixed_train)\n",
    "# test_url_counts = pt.make_url_counts(fixed_test)\n",
    "\n",
    "# test ulr fixer function\n",
    "# revised_train_lines = pt.replace_urls(train_url_counts)\n",
    "# revised_test_lines = pt.replace_urls(test_url_counts)\n",
    "# just look at training data\n",
    "# for revised_line in revised_train_lines[32:100]:\n",
    "#     print(revised_line)\n",
    "\n",
    "# write updated files\n",
    "# pt.write_lines_to_csv(list_of_lines = revised_train_lines, file_name = \"./data/train_clean_v02.csv\")\n",
    "# pt.write_lines_to_csv(list_of_lines = revised_test_lines, file_name = \"./data/test_clean_v02.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a857ecd-d87d-45d0-8905-50bd473e8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,keyword,location,text,target,url_count\n",
      "\n",
      "48,ablaze,Birmingham,at bbcmtd Wholesale Markets ablaze web link,1,1\n",
      "\n",
      "49,ablaze,Est. September 2012 - Bristol,We always try to bring the heavy. hash tag metal hash tag RT web link,0,1\n",
      "\n",
      "54,ablaze,Pretoria,at PhDSquares hash tag mufc they've built so much hype around new acquisitions but I doubt they will set the EPL ablaze this season.,0,0\n",
      "\n",
      "91,ablaze,\"Concord, CA\",at Navista7 Steve these fires out here are something else! California is a tinderbox - and this clown was setting my 'hood ablaze at News24680,1,0\n",
      "\n",
      "139,accident,\"Hagerstown, MD\",hash tag BREAKING: there was a deadly motorcycle car accident that happened to hash tag Hagerstown today. I'll have more details at 5 at Your4State. hash tag WHAG,1,0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace special twitter characters @ and # with markers (\"at\", \"hash tag\")\n",
    "with open(\"./data/train_clean_v02.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    revised_train_lines = f.readlines()\n",
    "\n",
    "with open(\"./data/test_clean_v02.csv\", encoding=\"utf8\", errors='ignore') as f:\n",
    "    revised_test_lines = f.readlines()\n",
    "\n",
    "fix_train_special_chars = pt.replace_twitter_specials(revised_train_lines)\n",
    "fix_test_special_chars = pt.replace_twitter_specials(revised_test_lines)\n",
    "\n",
    "# check that lines with @ and # are fixed\n",
    "#id = header 48   49 54  91  139\n",
    "for j in [0, 32, 33, 37, 64, 98]:\n",
    "    print(fix_train_special_chars[j])  # looks good\n",
    "\n",
    "fix_test_special_chars = pt.replace_twitter_specials(revised_test_lines)\n",
    "pt.write_lines_to_csv(list_of_lines = fix_train_special_chars, file_name = \"./data/train_clean_v03.csv\")\n",
    "pt.write_lines_to_csv(list_of_lines = fix_test_special_chars, file_name = \"./data/test_clean_v03.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# break out the text column so it can be operated on separately\n",
    "df_train04 = pd.read_csv('./data/train_clean_v03.csv', encoding=\"utf8\")\n",
    "df_test04 = pd.read_csv('./data/test_clean_v03.csv', encoding=\"utf8\")\n",
    "\n",
    "text_train = df_train04['text'].to_list()\n",
    "text_test = df_test04['text'].to_list()\n",
    "\n",
    "text_train04 = pt.remove_digits_and_punc(text_train)\n",
    "text_test04 = pt.remove_digits_and_punc(text_test)\n",
    "\n",
    "# replace text column with updated versions\n",
    "df_train04['text'] = text_train04\n",
    "df_test04['text'] = text_test04\n",
    "\n",
    "# write out updated data sets\n",
    "df_train04.to_csv('./data/train_clean_v04.csv', index=False, encoding=\"utf8\")\n",
    "df_test04.to_csv('./data/test_clean_v04.csv', index=False, encoding=\"utf8\")\n",
    "\n",
    "print(len(text_train), len(text_test), df_train04.shape, df_test04.shape)  # 7613 3263 (7613, 6) (3263, 5)\n",
    "df_train04.loc[20:40, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train04[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in training data as dataframe\n",
    "# import pandas as pd\n",
    "\n",
    "# df_train = pd.read_csv('./data/train_clean_v01.csv', encoding=\"utf8\")\n",
    "# print(df_train.shape)\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
