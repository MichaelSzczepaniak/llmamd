{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7613 labled tweets\n",
    "+ *test.csv* - 3236 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 6090 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1523 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e8d7b-78a3-4581-9ae2-688cdf694678",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Manual inspection of train.csv\n",
    "\n",
    "The following issues observered in the data are listed below.  They are numbered to indicate the order in which they were processed.  For example, spillover lines were fixed first.  This order is important because removing things like punctuation too early would make things like identifying user names or hashtags in a tweet impossible or make URLs invalid.\n",
    "\n",
    "### 1. Spillover lines\n",
    "\n",
    "The first issue we see with this data is that while most of the samples are on there own line, some spill over to adjacent lines. Here are few examples:\n",
    "\n",
    ">`61,ablaze,,\"on the outside you're ablaze and alive`  \n",
    ">`but you're dead inside\",0`  \n",
    ">`74,ablaze,India,\"Man wife get six years jail for setting ablaze niece`  \n",
    ">`http://t.co/eV1ahOUCZA\",1`  \n",
    ">`86,ablaze,Inang Pamantasan,\"Progressive greetings!`  \n",
    ">  \n",
    ">`In about a month students would have set their pens ablaze in The Torch Publications'... http://t.co/9FxPiXQuJt\",0`  \n",
    ">`117,accident,,\"mom: 'we didn't get home as fast as we wished'`  \n",
    ">`me: 'why is that?'`  \n",
    ">`mom: 'there was an accident and some truck spilt mayonnaise all over ??????\",0`\n",
    "\n",
    "The custom function `fix_spillover_lines` was written to fix these lines. Its code is available in the projtools module.\n",
    "\n",
    "### 2. Text-Target Duplicates\n",
    "\n",
    "If two or more rows in the original data have the same values in the `text` and `target` fields (columns), then these are considered **text-target duplicates**.  Because we are only using the content of the tweet (value in the `text` field) to classify it as **disaster** or **not disaster**, only one of these instances provides useful information for the model to learn.  For this reasons, only the first instance of these duplicates are retained and remainder are discarded from the training set.\n",
    "\n",
    "### 3. Cross-Target Duplicates\n",
    "\n",
    "When 2 rows in the data have the same `text` values (tweet content), but different values for `target`, these rows are considered **cross-target duplicates**.  Examples of these types of duplicates are shown below.  Since we don't know which tweet has the correct target value, all of these types of duplicates are removed from the training set.\n",
    "\n",
    "<img src='./visuals/cross_target_dupes.png'></img>\n",
    "\n",
    "### 4. Normalizing URLs\n",
    "\n",
    "Some tweet contain one or more URLs.  I assume that the content of a ULR does not contain any useful, but since a `<url>` token exists in the twitter gloVe embeddings, URLS will be replaced by this token.  \n",
    "\n",
    "Although the actual URL may not contain much useful information, the count of URLs occuring in a tweet may be a useful feature and are counted before they are normalized.  About 90% of the URLs in the training data are of the form `http://t.co/<10 digit hash>`. For example: `http://t.co/9FxPiXQuJt`.  In about 10% of cases, these URLs start with `https:\\\\`.\n",
    "\n",
    "The `replace_urls` function replaces each URL by the string \"<url>\" for the reasons stated above.\n",
    "\n",
    "### 5. Process Twitter-specifc characters\n",
    "\n",
    "Because the `@` and `#` characters have special meaning in tweets, they need to be processed before removing other punctuation.  When a `@<username>` is seen in a tweet, it is a reference to a user whose name is `username`.  When a `#<hashname>` is seen in a tweet, it specifies a hashtag which is a reference to all tweet tweets that use the `hashname` hashtag.  In processing these characters, `@<username>` is converted to `<user> username` and `#<hashname>` is converted to `<hashtag> hashname`.  These replacement tokens were selected because they also have mappings in the embeddings file described in the **Normalizing URLs** section.\n",
    "\n",
    "### 6. Expanding Contractions\n",
    "\n",
    "Contraction fragments are included as vectorized tokens in the twitter gloVe embeddings which means that we don't need to expand these manually.  The spaCy tokenizer will separate the first word from the contraction fragments: e.g \"you're\" will be tokenized into `[\"you\", \"'re\"]`.  Because the embeddings file has a listing for the contraction fragment token `'re` (as well as other contraction fragments such as 'm, 's, 'll, etc.), we don't need to convert these to their actual word forms (e.g. \"am\", \"is\", \"will\", etc.) before vectorizing.\n",
    "\n",
    "### 7. Normalize digits, remove stop words, lemmatize, make lower case and remove punctuation\n",
    "\n",
    "The function `spacy_digits_and_stops` does all the tasks listed in this section and stores the lemmatized text as lower case as a final step.  Digit normalization is done by replacing sets of consecutive digits with the token `<number>`.  As with the other token replacements, this one also was chosen because it has a mapping in the embeddings file.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31e856-f8dc-43a4-b309-a6865c1c7942",
   "metadata": {},
   "source": [
    "## Test the complete pre-processing pipeline\n",
    "\n",
    "The augmented data doesn't need the following steps that the original data does:\n",
    "\n",
    "1. Fix spillover lines\n",
    "2. Fix text-target duplicates\n",
    "3. Fix cross-over target duplicates\n",
    "\n",
    "The following pre-processing steps are shared between the augmented and original data:\n",
    "\n",
    "4. Replace URLs with <url> token\n",
    "5. Replace the twitter-specific characters @ with <user> and # with <hashtag>\n",
    "6. Expand the contractions\n",
    "7. Remove digits, stop words, punctuation and make everything lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a19267b-17c9-4989-a4d8-1a394f80fcf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# df_train_v03 = pd.read_csv(\"./data/train_clean_v03.csv\", encoding=\"utf8\")\n",
    "# df_test_v01 = pd.read_csv(\"./data/test_clean_v01.csv\", encoding=\"utf8\")\n",
    "# print(df_train_v03.shape, df_test_v01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af35548c-01b8-40fe-aa50-c1b59e1b1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the following from the default stop word list\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# not_stops = {\"you\", \"on\", \"not\", \"from\", \"was\", \"but\", \"your\", \"all\", \"no\", \"when\",\n",
    "#              \"now\", \"more\", \"over\", \"some\", \"first\", \"full\", \"down\", \"may\", \"only\",\n",
    "#              \"last\", \"many\", \"never\", \"any\", \"everyone\", \"every\", \"before\", \"under\",\n",
    "#              \"top\", \"most\", \"during\", \"next\", \"while\", \"call\", \"very\", \"nothing\", \n",
    "#               \"anything\", \"everything\", \"sometimes\", \"serious\", \"everywhere\", \"none\",\n",
    "#               \"except\", \"within\", \"above\", \"below\", \"nobody\", \"afterwards\", \"anywhere\"}\n",
    "# nlp.Defaults.stop_words -= not_stops  # nlp instatiated cell 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import projtools as pt\n",
    "\n",
    "# train_v03_id = df_train_v03['id'].tolist()\n",
    "# train_v03_keyword = df_train_v03['keyword'].tolist()\n",
    "# train_v03_location = df_train_v03['location'].tolist()\n",
    "# train_v03_text = df_train_v03['text'].tolist()  # only field pipeline manipulates\n",
    "# train_v03_target = df_train_v03['target'].tolist()\n",
    "\n",
    "# train_v04_text_urls_fixed = pt.replace_urls(train_v03_text)\n",
    "# train_v05_text_fixed = pt.replace_twitter_specials(train_v04_text_urls_fixed)\n",
    "# train_v06_text_fixed = pt.expand_contractions(train_v05_text_fixed)\n",
    "# df_contractions_expanded = pd.DataFrame({'id': train_v03_id,\n",
    "#                                          'text': train_v06_text_fixed})\n",
    "# dict_df_stops = pt.spacy_digits_and_stops(df_contractions_expanded)\n",
    "# df_train_clean_v07 = dict_df_stops['df']\n",
    "# stops_removed = dict_df_stops['stops_removed']\n",
    "# train_clean_v07_text_fixed = df_train_clean_v07['text'].tolist()\n",
    "\n",
    "# df_train_v07_full_pipe = pd.DataFrame({'id': train_v03_id,\n",
    "#                                        'keyword': train_v03_keyword,\n",
    "#                                        'location': train_v03_location,\n",
    "#                                        'text': train_clean_v07_text_fixed,\n",
    "#                                        'target': train_v03_target})\n",
    "\n",
    "# df_train_v07_full_pipe.to_csv(path_or_buf=\"./data/train_clean_v07b.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc889feb-daa3-4604-8bdf-e0dbe5066e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cramer', 'land', 'fran', 'happily', 'apollo', 'entertainment', 'episode', 'gtfo', 'throw', 'cruise', 'talk', 'mumbai', 'lion', 'encore', 'will', 'guillermo', 'lighten', 'stressful', 'marshall', 'trolley']\n"
     ]
    }
   ],
   "source": [
    "vocab = pt.get_vocabulary()\n",
    "print(len(vocab))  # should be 4872\n",
    "print(list(vocab)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f9528e3-3057-4d68-b78a-1671de5320d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_oov_tokens - size of vocabulary: 4872\n"
     ]
    }
   ],
   "source": [
    "# the function should give the same results\n",
    "df_train_v08_full_pipe_function = pt.preproccess_pipeline(\"./data/train_clean_v03.csv\", \"./data/train_clean_v08b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c4a1f11-241e-4891-8982-7320671fc037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_oov_tokens - size of vocabulary: 4872\n"
     ]
    }
   ],
   "source": [
    "df_test_v08_full_pipe_function = pt.preproccess_pipeline(path_to_input_df_file=\"./data/test_clean_v01.csv\",\n",
    "                                                         path_to_output =\"./data/test_clean_v08b.csv\", isTrain=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c855c-8d45-43b6-b70a-7828145a4cd6",
   "metadata": {},
   "source": [
    "## Running the augmented tweets through the pipeline and next steps\n",
    "\n",
    "A diff between the `./data/train_clean_v07b.csv` and `./data/train_clean_v07c.csv` files indicates they are identical, so the next steps are:\n",
    "\n",
    "1. Run the augmented tweets through the same pipeline as the original data\n",
    "2. Tokenize the augmented tweets and look at the distribution of token counts per tweet\n",
    "3. From 2., determine the number of tokens used for the model input\n",
    "4. Train two logistic regression models:  `mod_logreg_orig` and `mod_logreg_aug` using just the orignal data and using the original + augmented data respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893bfa2e-f68c-48af-98f2-a15f96461c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b9381d-d23f-479b-967d-0fe1d0a25395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove_oov_tokens - size of vocabulary: 4872\n"
     ]
    }
   ],
   "source": [
    "# run augmented tweets through the pipeline\n",
    "# df_aug_v07_full_pipe = pt.preproccess_pipeline(\"./data/prompts_v05/aug_tweets_v05prompt_all_.csv\", \"./data/aug_clean_v07.csv\", True)\n",
    "df_aug_v08_full_pipe = pt.preproccess_pipeline(path_to_input_df_file=\"./data/prompts_v05/aug_tweets_v05prompt_all_.csv\",\n",
    "                                               path_to_output=\"./data/aug_clean_v08.csv\", isAugmented=True, isTrain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626be46a-4ce6-4948-ac1d-e2ff5ee0ca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>witness devastation cause powerful hurricane caribbean region pray safety all affect &lt;hashtag&gt; hurricane &lt;hashtag&gt; caribbean</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20004</td>\n",
       "      <td>devastate forest fire near la heart go all affect tragic disaster &lt;hashtag&gt; &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20005</td>\n",
       "      <td>break news authority local shelter place wildfire continue part california stay safe prepare &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20006</td>\n",
       "      <td>massive flooding force &lt;number&gt; resident evacuate rescue effort underway home water &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20007</td>\n",
       "      <td>receive video from &lt;hashtag&gt; california flood water engulf neighborhood stay safe everyone &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20008</td>\n",
       "      <td>&lt;hashtag&gt; leave destruction wake think prayer affect &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20010</td>\n",
       "      <td>rain trigger flash flood philippine lead chaos city street stay safe everyone &lt;hashtag&gt; &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20013</td>\n",
       "      <td>on coast tsunami approach fast pray everyone safety stay safe stay alert &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20014</td>\n",
       "      <td>massive wildfire force evacuation entire town california think prayer all affect disaster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20015</td>\n",
       "      <td>battle rage wildfire town feel like lose battle stay safe everyone &lt;hashtag&gt; disaster &lt;hashtag&gt; wildfire</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  20001   \n",
       "1  20004   \n",
       "2  20005   \n",
       "3  20006   \n",
       "4  20007   \n",
       "5  20008   \n",
       "6  20010   \n",
       "7  20013   \n",
       "8  20014   \n",
       "9  20015   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "0  witness devastation cause powerful hurricane caribbean region pray safety all affect <hashtag> hurricane <hashtag> caribbean   \n",
       "1                                         devastate forest fire near la heart go all affect tragic disaster <hashtag> <hashtag>   \n",
       "2                        break news authority local shelter place wildfire continue part california stay safe prepare <hashtag>   \n",
       "3                                 massive flooding force <number> resident evacuate rescue effort underway home water <hashtag>   \n",
       "4                          receive video from <hashtag> california flood water engulf neighborhood stay safe everyone <hashtag>   \n",
       "5                                                                <hashtag> leave destruction wake think prayer affect <hashtag>   \n",
       "6                             rain trigger flash flood philippine lead chaos city street stay safe everyone <hashtag> <hashtag>   \n",
       "7                                            on coast tsunami approach fast pray everyone safety stay safe stay alert <hashtag>   \n",
       "8                                     massive wildfire force evacuation entire town california think prayer all affect disaster   \n",
       "9                      battle rage wildfire town feel like lose battle stay safe everyone <hashtag> disaster <hashtag> wildfire   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  \n",
       "5       1  \n",
       "6       1  \n",
       "7       1  \n",
       "8       1  \n",
       "9       1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "# df_aug_v07_full_pipe.head(10)\n",
    "df_aug_v08_full_pipe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae02e2f-f4cd-4cd8-a61d-039719a2aa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a83fb-0682-4240-a7bb-045b6257aa73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dc288f-2dc7-4e4a-b244-f2c1eb632ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8912a7-c88f-4d9a-b12a-d868bb18ae3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7672dc4-42fc-45f8-8aec-31a1755d89e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7641e-4723-4cf3-b5a3-87377bc38c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
