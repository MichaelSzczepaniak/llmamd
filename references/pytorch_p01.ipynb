{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e83557-9ca3-4865-bdd4-a0ff8f3554ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3471,  0.4547, -0.2356]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # missing from lecture 2\n",
    "import torch.nn as nn\n",
    "\n",
    "## Create input_tensor with three features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "print(input_tensor.shape)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdcd661-3b6c-42eb-8899-c7923ca5ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6237,  0.6049]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define a linear layer\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8cf544-effc-47bd-8152-e5e1199caf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.4013, -0.4596,  0.4234],\n",
       "        [-0.1100,  0.5659,  0.2756]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(linear_layer.weight.shape)\n",
    "linear_layer.weight\n",
    "# Parameter containing:\n",
    "# tensor([[ 0.1666,  0.4067, -0.4190],\n",
    "#         [-0.1845,  0.0945, -0.2244]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899ebdeb-3c02-4161-a38a-cf00383eed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.4542,  0.4507], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias\n",
    "# Parameter containing:\n",
    "# tensor([-0.2581, -0.1212], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9086b1-cf4e-4d21-be5d-92367178a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7038]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ex 2\n",
    "# Create a neural network of linear layers that takes a tensor of dimensions 1x8 as input and outputs a tensor of dimensions 1x1\n",
    "# Use any output dimension for the first layer you want.\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8, 3),  # using 3 as the output dim of the first layer, but could be anything\n",
    "                      nn.Linear(3, 1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)  # 1x1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667ebb16-37a2-4da5-811f-9920cd334387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9975]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid activation\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b38b9b-52da-4d89-b582-f4d360c1e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "# softmax for multi-dim classification\n",
    "# Create an input tensor\n",
    "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "# Apply softmax along the last dimension\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e6f888-97ce-44f9-abc0-19c9d2e3edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4184],\n",
      "        [0.7402],\n",
      "        [0.6142],\n",
      "        [0.6220],\n",
      "        [0.5035]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Binary classification: forward pass\n",
    "# Create input data of shape 5x6\n",
    "input_data = torch.tensor(\n",
    "[[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "[-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
    "[ 0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n",
    "[-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
    "[-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]])\n",
    "\n",
    "# Create binary classification model\n",
    "model = nn.Sequential(\n",
    "nn.Linear(6, 4), # First linear layer\n",
    "nn.Linear(4, 1), # Second linear layer\n",
    "nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c2125c-855a-4db8-9584-9fc2d2ad85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0075]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lecture 4 excercises\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34aa492d-ccb5-4b4a-a354-c11ec660ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0024]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with exactly four linear layers, which takes\n",
    "# the input tensor as input, and outputs a regression value, using any\n",
    "# shapes you like for the hidden layers.\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 7),\n",
    "    nn.Linear(7, 6),\n",
    "    nn.Linear(6, 5),\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61844f91-412b-492a-9996-547c49da5c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1511, 0.1212, 0.2878, 0.4399]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A similar neural network to the one you just built is provided, containing four linear layers;\n",
    "# update this network to perform a multi-class classification with four outputs.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b7c1c1-c4fc-47ff-9eff-abf0dfce5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde3cf5a-40ce-4dc3-bdde-f4f3d98df345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# one-hot encoding\n",
    "print(F.one_hot(torch.tensor(0), num_classes = 3))\n",
    "print(F.one_hot(torch.tensor(1), num_classes = 3))\n",
    "print(F.one_hot(torch.tensor(2), num_classes = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93a008de-1a80-4bc8-968a-f21b8bffb590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1, 0]])\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e889cc6-6a63-4a1b-9d16-d81c10f333e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.443492440036494 0.5565075599635061\n"
     ]
    }
   ],
   "source": [
    "# try it manually, start with softmax probabilities\n",
    "import numpy as np\n",
    "scores = [-0.1211, 0.1059]\n",
    "targets = [1, 0]\n",
    "denom = np.exp(scores[0]) + np.exp(scores[1])\n",
    "p0 = np.exp(scores[0]) / denom\n",
    "p1 = np.exp(scores[1]) / denom\n",
    "print(p0, p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb2ffd4-feea-4f00-9a61-85757b2fd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8130745235187279\n"
     ]
    }
   ],
   "source": [
    "# simplified for single point, binary classification\n",
    "# for the general multi-nomial version see: https://rpubs.com/mszczepaniak/classificationgoodness\n",
    "cross_entropy = -(np.log(p0))\n",
    "print(cross_entropy)  # very close: 0.81307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9773deef-5830-49f0-9f69-62ceab088415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n",
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# lecture 5 excercises\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(1), num_classes)\n",
    "print(one_hot_numpy)\n",
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd5447-e06f-4129-a7e4-c2306e4cc44e",
   "metadata": {},
   "source": [
    "Start by creating a one-hot encoded vector of the ground truth label y, which is a required step to compare y with the scores predicted by your model. Next, you'll create a cross entropy loss function. Last, you'll call the loss function, which takes scores (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d7d0d1-a039-49fc-bd8f-73f7c9054b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0]])\n",
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes = scores.shape[1])\n",
    "print(one_hot_label)\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f2d3071-7d04-4ec1-9ecb-213af59005ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 6 - Backpropagation in PyTorch\n",
    "# exercise 1\n",
    "weight = torch.tensor([[-1.5349,  1.1395, -0.7303, -1.4280,  1.4367,  1.5740,  0.8475,  1.3379, 1.5674],\n",
    "                       [ 1.1543,  1.2855,  0.5122,  0.4215,  0.4982,  1.5640, -0.9705, -0.4987, 0.6331]])\n",
    "bias = torch.tensor([-0.1435,  1.6498])\n",
    "preds = torch.tensor([[2.1289, 3.7059]])\n",
    "target = torch.tensor([[1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2746d1-ce46-4a08-a480-40e339fe10aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "# torch.set_grad_enabled(True)  # Context-manager \n",
    "loss.requires_grad = True\n",
    "# loss.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82649456-5264-4fd8-affb-2c3fc1532b4f",
   "metadata": {},
   "source": [
    "This is the output I get in the DataCamp environment:\n",
    "\n",
    "`tensor([[-0.5063, -0.4353, -0.3859, -0.3938, -0.5257, -0.1628, -0.5167, -0.4315,\n",
    "         -0.6264],\n",
    "        [ 0.5063,  0.4353,  0.3859,  0.3938,  0.5257,  0.1628,  0.5167,  0.4315,\n",
    "          0.6264]])\n",
    "tensor([-0.8288,  0.8288])\n",
    "\n",
    "<script.py> output:\n",
    "    tensor([[-0.5215, -0.4484, -0.3975, -0.4056, -0.5416, -0.1677, -0.5322, -0.4444,\n",
    "             -0.6453],\n",
    "            [ 0.5215,  0.4484,  0.3975,  0.4056,  0.5416,  0.1677,  0.5322,  0.4444,\n",
    "              0.6453]])\n",
    "    tensor([-0.8537,  0.8537])`\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e777f976-403e-4982-9e26-7837d8a3abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f554c498-b923-4500-8355-2658ca3fff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1071,  0.2024, -0.2384, -0.0131,  0.2494, -0.1972, -0.1973, -0.2014,\n",
      "         -0.2002,  0.2105, -0.1943, -0.0190, -0.2011, -0.2072,  0.1547,  0.0311],\n",
      "        [ 0.0141,  0.1303,  0.1676, -0.1798, -0.1731,  0.0695, -0.1607, -0.2313,\n",
      "          0.2398,  0.1843,  0.0763, -0.2182,  0.1072,  0.0588,  0.2093, -0.2236],\n",
      "        [ 0.0103, -0.1128, -0.1181, -0.0694,  0.2302, -0.1433, -0.0747, -0.1812,\n",
      "         -0.2022, -0.1604, -0.1837, -0.1262, -0.1441, -0.2161, -0.1814,  0.1407],\n",
      "        [-0.1892,  0.0483, -0.2183, -0.2134,  0.1870,  0.1905,  0.1514, -0.2494,\n",
      "         -0.2193,  0.1960, -0.1626, -0.2326, -0.2115,  0.1810,  0.2430,  0.2214],\n",
      "        [-0.1015, -0.1876,  0.2387,  0.0375,  0.1484, -0.2328, -0.2265,  0.1146,\n",
      "          0.1025, -0.2453,  0.2474, -0.0629,  0.1831,  0.0089, -0.2116, -0.2073],\n",
      "        [ 0.1988,  0.2435,  0.0044, -0.1996, -0.1659,  0.0005, -0.1540, -0.0734,\n",
      "         -0.1695, -0.2391, -0.0378, -0.0973, -0.2471,  0.2273, -0.0283, -0.0063],\n",
      "        [ 0.2490,  0.0579,  0.0973,  0.1275,  0.1136,  0.1962,  0.1423,  0.0847,\n",
      "          0.0993,  0.0854,  0.0480, -0.0795, -0.0933, -0.0690,  0.0394,  0.0831],\n",
      "        [ 0.1630,  0.1355,  0.2213,  0.2339,  0.1554, -0.1767,  0.2188,  0.1079,\n",
      "          0.1947,  0.1080,  0.1223,  0.1989, -0.2247,  0.2416,  0.1834, -0.0096]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=16, out_features=8, bias=True),\n",
    "    nn.Linear(in_features=8, out_features=4, bias=True),\n",
    "    nn.Linear(in_features=4, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "print(weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2472110f-59ee-4bf4-b4aa-77bbcd229a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "print(grads0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04384f16-90e2-4d5d-9125-ff7f6101e693",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Update the weights using the learning rate and the gradients\u001b[39;00m\n\u001b[0;32m     17\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m---> 18\u001b[0m weight0 \u001b[38;5;241m=\u001b[39m weight0 \u001b[38;5;241m-\u001b[39m (\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrads0\u001b[49m)\n\u001b[0;32m     19\u001b[0m weight1 \u001b[38;5;241m=\u001b[39m weight1 \u001b[38;5;241m-\u001b[39m (lr \u001b[38;5;241m*\u001b[39m grads1)\n\u001b[0;32m     20\u001b[0m weight2 \u001b[38;5;241m=\u001b[39m weight2 \u001b[38;5;241m-\u001b[39m (lr \u001b[38;5;241m*\u001b[39m grads2)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Update the weights using the learning rate and the gradients\n",
    "lr = 0.001\n",
    "weight0 = weight0 - (lr * grads0)\n",
    "weight1 = weight1 - (lr * grads1)\n",
    "weight2 = weight2 - (lr * grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e11a7885-3cd3-43e2-bca0-773cdba8a836",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1.\u001b[39m, \u001b[38;5;241m0.\u001b[39m]])\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, target)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Update the model's parameters using the optimizer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mC:\\VirtualEnvironments\\llmamd\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\VirtualEnvironments\\llmamd\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "pred = torch.tensor([[-0.1738,  0.1308]])\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "loss.backward()   # RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15af4e51-71d1-467c-b3c9-066ab701b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "# lecture 7 exercises\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0022d867-7729-4445-9542-c1d109d54fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=4, out_features=2, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=2, out_features=1, bias=True)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cd098-6a1a-46c9-ba8d-7d82830fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, dataloader):\n",
    "    model.eval()\n",
    "    iter_loader = iter(dataloader)\n",
    "    for _ in range(3):\n",
    "        feature, target = next(iter_loader)\n",
    "        preds = model(feature)\n",
    "        for p, t in zip(preds, target):\n",
    "            print(f'Ground truth salary: {t.item():.3f}. Predicted salary: {p.item():.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8047c1-fea7-4fec-ae8b-b2e5c78b9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load data for this to work locally\n",
    "num_epochs = 10\n",
    "# Loop over the number of epochs and the dataloader\n",
    "for i in range(num_epochs):\n",
    "  for data in dataloader:\n",
    "    # Set the gradients to zero\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    feature, target = data\n",
    "    prediction = model(feature)\n",
    "    # Calculate the loss\n",
    "    loss = criterion(prediction, target)\n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    # Update the model's parameters\n",
    "    optimizer.step()\n",
    "\n",
    "show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a70975-ab20-41cc-8160-58e3564718b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8998f2-cb96-454d-83f4-b54655119ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386d3fd-f176-47e0-ac7b-43ca0ba3963a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce62547-6d05-49ae-95ce-332f302dbf30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bfeb1e-a94f-4e46-89d0-392e3d1d63de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e071bf4-e990-49b1-9cc1-29833ec60458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c23b4-5333-4720-a145-b442bcd7040c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334c8f4-8e5e-4123-aea7-9242656dfb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d1650-5a92-46ee-948e-9ba5dd213515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
