{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e83557-9ca3-4865-bdd4-a0ff8f3554ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3471,  0.4547, -0.2356]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch  # missing from lecture 2\n",
    "import torch.nn as nn\n",
    "\n",
    "## Create input_tensor with three features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])\n",
    "print(input_tensor.shape)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdcd661-3b6c-42eb-8899-c7923ca5ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6837, -0.3210]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# define a linear layer\n",
    "linear_layer = nn.Linear(in_features=3, out_features=2)\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8cf544-effc-47bd-8152-e5e1199caf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1789,  0.4501, -0.4519],\n",
       "        [-0.2985, -0.4944,  0.2654]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(linear_layer.weight.shape)\n",
    "linear_layer.weight\n",
    "# Parameter containing:\n",
    "# tensor([[ 0.1666,  0.4067, -0.4190],\n",
    "#         [-0.1845,  0.0945, -0.2244]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899ebdeb-3c02-4161-a38a-cf00383eed48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4347, 0.0700], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.bias\n",
    "# Parameter containing:\n",
    "# tensor([-0.2581, -0.1212], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba9086b1-cf4e-4d21-be5d-92367178a718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4369]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ex 2\n",
    "# Create a neural network of linear layers that takes a tensor of dimensions 1x8 as input and outputs a tensor of dimensions 1x1\n",
    "# Use any output dimension for the first layer you want.\n",
    "input_tensor = torch.Tensor([[2, 3, 6, 7, 9, 3, 2, 1]])\n",
    "\n",
    "# Implement a small neural network with exactly two linear layers\n",
    "model = nn.Sequential(nn.Linear(8, 3),  # using 3 as the output dim of the first layer, but could be anything\n",
    "                      nn.Linear(3, 1)\n",
    "                     )\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)  # 1x1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667ebb16-37a2-4da5-811f-9920cd334387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9975]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid activation\n",
    "input_tensor = torch.tensor([[6.0]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b38b9b-52da-4d89-b582-f4d360c1e17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "# softmax for multi-dim classification\n",
    "# Create an input tensor\n",
    "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "# Apply softmax along the last dimension\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "output_tensor = probabilities(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e6f888-97ce-44f9-abc0-19c9d2e3edb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6702],\n",
      "        [0.4348],\n",
      "        [0.5445],\n",
      "        [0.4950],\n",
      "        [0.5525]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Binary classification: forward pass\n",
    "# Create input data of shape 5x6\n",
    "input_data = torch.tensor(\n",
    "[[-0.4421, 1.5207, 2.0607, -0.3647, 0.4691, 0.0946],\n",
    "[-0.9155, -0.0475, -1.3645, 0.6336, -1.9520, -0.3398],\n",
    "[ 0.7406, 1.6763, -0.8511, 0.2432, 0.1123, -0.0633],\n",
    "[-1.6630, -0.0718, -0.1285, 0.5396, -0.0288, -0.8622],\n",
    "[-0.7413, 1.7920, -0.0883, -0.6685, 0.4745, -0.4245]])\n",
    "\n",
    "# Create binary classification model\n",
    "model = nn.Sequential(\n",
    "nn.Linear(6, 4), # First linear layer\n",
    "nn.Linear(4, 1), # Second linear layer\n",
    "nn.Sigmoid() # Sigmoid activation function\n",
    ")\n",
    "# Pass input data through model\n",
    "output = model(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c2125c-855a-4db8-9584-9fc2d2ad85fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0019]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# lecture 4 excercises\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a small neural network for binary classification\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(8, 1),\n",
    "  nn.Sigmoid()\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34aa492d-ccb5-4b4a-a354-c11ec660ff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7475]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with exactly four linear layers, which takes\n",
    "# the input tensor as input, and outputs a regression value, using any\n",
    "# shapes you like for the hidden layers.\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 7),\n",
    "    nn.Linear(7, 6),\n",
    "    nn.Linear(6, 5),\n",
    "    nn.Linear(5, 1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61844f91-412b-492a-9996-547c49da5c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0636, 0.2537, 0.4786, 0.2041]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A similar neural network to the one you just built is provided, containing four linear layers;\n",
    "# update this network to perform a multi-class classification with four outputs.\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Update network below to perform a multi-class classification with four labels\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(11, 20),\n",
    "  nn.Linear(20, 12),\n",
    "  nn.Linear(12, 6),\n",
    "  nn.Linear(6, 4), \n",
    "  nn.Softmax(dim=-1)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44b7c1c1-c4fc-47ff-9eff-abf0dfce5959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cde3cf5a-40ce-4dc3-bdde-f4f3d98df345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n",
      "tensor([0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# one-hot encoding\n",
    "print(F.one_hot(torch.tensor(0), num_classes = 3))\n",
    "print(F.one_hot(torch.tensor(1), num_classes = 3))\n",
    "print(F.one_hot(torch.tensor(2), num_classes = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93a008de-1a80-4bc8-968a-f21b8bffb590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8131, dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "scores = torch.tensor([[-0.1211, 0.1059]])\n",
    "one_hot_target = torch.tensor([[1, 0]])\n",
    "criterion = CrossEntropyLoss()\n",
    "criterion(scores.double(), one_hot_target.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e889cc6-6a63-4a1b-9d16-d81c10f333e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.443492440036494 0.5565075599635061\n"
     ]
    }
   ],
   "source": [
    "# try it manually, start with softmax probabilities\n",
    "import numpy as np\n",
    "scores = [-0.1211, 0.1059]\n",
    "targets = [1, 0]\n",
    "denom = np.exp(scores[0]) + np.exp(scores[1])\n",
    "p0 = np.exp(scores[0]) / denom\n",
    "p1 = np.exp(scores[1]) / denom\n",
    "print(p0, p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cb2ffd4-feea-4f00-9a61-85757b2fd347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8130745235187279\n"
     ]
    }
   ],
   "source": [
    "# simplified for single point, binary classification\n",
    "# for the general multi-nomial version see: https://rpubs.com/mszczepaniak/classificationgoodness\n",
    "cross_entropy = -(np.log(p0))\n",
    "print(cross_entropy)  # very close: 0.81307"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9773deef-5830-49f0-9f69-62ceab088415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0]\n",
      "tensor([0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# lecture 5 excercises\n",
    "y = 1\n",
    "num_classes = 3\n",
    "\n",
    "# Create the one-hot encoded vector using NumPy\n",
    "one_hot_numpy = np.array([0, 1, 0])\n",
    "\n",
    "# Create the one-hot encoded vector using PyTorch\n",
    "one_hot_pytorch = F.one_hot(torch.tensor(1), num_classes)\n",
    "print(one_hot_numpy)\n",
    "print(one_hot_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd5447-e06f-4129-a7e4-c2306e4cc44e",
   "metadata": {},
   "source": [
    "Start by creating a one-hot encoded vector of the ground truth label y, which is a required step to compare y with the scores predicted by your model. Next, you'll create a cross entropy loss function. Last, you'll call the loss function, which takes scores (model predictions before the final softmax function), and the one-hot encoded ground truth label, as inputs. It outputs a single float, the loss of that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d7d0d1-a039-49fc-bd8f-73f7c9054b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 1, 0]])\n",
      "tensor(8.0619, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "y = [2]\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes = scores.shape[1])\n",
    "print(one_hot_label)\n",
    "# Create the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Calculate the cross entropy loss\n",
    "loss = criterion(scores.double(), one_hot_label.double())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f2d3071-7d04-4ec1-9ecb-213af59005ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture 6 - Backpropagation in PyTorch\n",
    "# exercise 1\n",
    "weight = torch.tensor([[-1.5349,  1.1395, -0.7303, -1.4280,  1.4367,  1.5740,  0.8475,  1.3379, 1.5674],\n",
    "                       [ 1.1543,  1.2855,  0.5122,  0.4215,  0.4982,  1.5640, -0.9705, -0.4987, 0.6331]])\n",
    "bias = torch.tensor([-0.1435,  1.6498])\n",
    "preds = torch.tensor([[2.1289, 3.7059]])\n",
    "target = torch.tensor([[1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2746d1-ce46-4a08-a480-40e339fe10aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate the loss\n",
    "loss = criterion(preds, target)\n",
    "\n",
    "# Compute the gradients of the loss\n",
    "# torch.set_grad_enabled(True)  # Context-manager \n",
    "loss.requires_grad = True\n",
    "# loss.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Display gradients of the weight and bias tensors in order\n",
    "print(weight.grad)\n",
    "print(bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82649456-5264-4fd8-affb-2c3fc1532b4f",
   "metadata": {},
   "source": [
    "This is the output I get in the DataCamp environment:\n",
    "\n",
    "`tensor([[-0.5063, -0.4353, -0.3859, -0.3938, -0.5257, -0.1628, -0.5167, -0.4315,\n",
    "         -0.6264],\n",
    "        [ 0.5063,  0.4353,  0.3859,  0.3938,  0.5257,  0.1628,  0.5167,  0.4315,\n",
    "          0.6264]])\n",
    "tensor([-0.8288,  0.8288])\n",
    "\n",
    "<script.py> output:\n",
    "    tensor([[-0.5215, -0.4484, -0.3975, -0.4056, -0.5416, -0.1677, -0.5322, -0.4444,\n",
    "             -0.6453],\n",
    "            [ 0.5215,  0.4484,  0.3975,  0.4056,  0.5416,  0.1677,  0.5322,  0.4444,\n",
    "              0.6453]])\n",
    "    tensor([-0.8537,  0.8537])`\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e777f976-403e-4982-9e26-7837d8a3abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 8),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Linear(8, 2))\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f554c498-b923-4500-8355-2658ca3fff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1474, -0.0907,  0.0355, -0.0034, -0.2320, -0.0343, -0.0239,  0.1238,\n",
      "          0.0715,  0.2001,  0.1781,  0.1280, -0.0649,  0.0769,  0.0991,  0.0659],\n",
      "        [ 0.0124, -0.1377,  0.0165, -0.0508,  0.1346, -0.0372, -0.2265, -0.0657,\n",
      "         -0.1941,  0.0185,  0.0895,  0.1059,  0.2352,  0.0381,  0.1332,  0.0310],\n",
      "        [ 0.0496, -0.1836,  0.1762, -0.0700, -0.1048, -0.1848,  0.1856, -0.0505,\n",
      "         -0.1311, -0.1946, -0.1530, -0.2420, -0.0441,  0.0900, -0.0885, -0.0771],\n",
      "        [-0.0822, -0.2342, -0.1330,  0.0716, -0.1040, -0.0387,  0.1673,  0.0490,\n",
      "         -0.0978, -0.0585,  0.1109,  0.0281,  0.0678,  0.0737, -0.1800, -0.1981],\n",
      "        [ 0.2351,  0.0743, -0.2024,  0.1219, -0.1569,  0.2129, -0.1537,  0.0374,\n",
      "         -0.0047, -0.0469, -0.1243, -0.0908,  0.0253, -0.1333,  0.1673,  0.2139],\n",
      "        [ 0.0063,  0.2349,  0.1194,  0.1672, -0.1216,  0.2160, -0.2494,  0.2291,\n",
      "          0.1965, -0.0401,  0.1115,  0.0607, -0.1332,  0.0415,  0.0129, -0.1823],\n",
      "        [-0.1562, -0.1449, -0.0288,  0.0617,  0.2332, -0.0013,  0.1606, -0.1243,\n",
      "          0.1395,  0.0656, -0.1289, -0.1164, -0.1913,  0.1013,  0.1484, -0.0465],\n",
      "        [-0.1018, -0.1230,  0.2043, -0.1373,  0.1081,  0.1287, -0.1319, -0.0496,\n",
      "          0.1697, -0.1640, -0.1587, -0.1286, -0.1029,  0.0181,  0.1569,  0.2335]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=16, out_features=8, bias=True),\n",
    "    nn.Linear(in_features=8, out_features=4, bias=True),\n",
    "    nn.Linear(in_features=4, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "weight0 = model[0].weight\n",
    "weight1 = model[1].weight\n",
    "weight2 = model[2].weight\n",
    "\n",
    "print(weight0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2472110f-59ee-4bf4-b4aa-77bbcd229a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Access the gradients of the weight of each linear layer\n",
    "grads0 = weight0.grad\n",
    "grads1 = weight1.grad\n",
    "grads2 = weight2.grad\n",
    "\n",
    "print(grads0)  # FIXME: why getting None?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04384f16-90e2-4d5d-9125-ff7f6101e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the weights using the learning rate and the gradients\n",
    "lr = 0.001\n",
    "# FIXME\n",
    "# weight0 = weight0 - (lr * grads0)\n",
    "# weight1 = weight1 - (lr * grads1)\n",
    "# weight2 = weight2 - (lr * grads2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e11a7885-3cd3-43e2-bca0-773cdba8a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "pred = torch.tensor([[-0.1738,  0.1308]])\n",
    "target = torch.tensor([[1., 0.]])\n",
    "\n",
    "loss = criterion(pred, target)\n",
    "# loss.backward()   # FIXME: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n",
    "\n",
    "# Update the model's parameters using the optimizer\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15af4e51-71d1-467c-b3c9-066ab701b858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(81.)\n"
     ]
    }
   ],
   "source": [
    "# lecture 7 exercises\n",
    "y_hat = np.array(10)\n",
    "y = np.array(1)\n",
    "\n",
    "# Calculate the MSELoss using NumPy\n",
    "mse_numpy = np.mean((y_hat - y)**2)\n",
    "\n",
    "# Create the MSELoss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Calculate the MSELoss using the created loss function\n",
    "mse_pytorch = criterion(torch.tensor(y_hat).float(), torch.tensor(y).float())\n",
    "print(mse_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0022d867-7729-4445-9542-c1d109d54fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=4, out_features=2, bias=True),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(in_features=2, out_features=1, bias=True)\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783cd098-6a1a-46c9-ba8d-7d82830fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, dataloader):\n",
    "    model.eval()\n",
    "    iter_loader = iter(dataloader)\n",
    "    for _ in range(3):\n",
    "        feature, target = next(iter_loader)\n",
    "        preds = model(feature)\n",
    "        for p, t in zip(preds, target):\n",
    "            print(f'Ground truth salary: {t.item():.3f}. Predicted salary: {p.item():.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e8047c1-fea7-4fec-ae8b-b2e5c78b9854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load data for this to work locally\n",
    "# num_epochs = 10\n",
    "# # Loop over the number of epochs and the dataloader\n",
    "# for i in range(num_epochs):\n",
    "#   for data in dataloader:\n",
    "#     # Set the gradients to zero\n",
    "#     optimizer.zero_grad()\n",
    "#     # Run a forward pass\n",
    "#     feature, target = data\n",
    "#     prediction = model(feature)\n",
    "#     # Calculate the loss\n",
    "#     loss = criterion(prediction, target)\n",
    "#     # Compute the gradients\n",
    "#     loss.backward()\n",
    "#     # Update the model's parameters\n",
    "#     optimizer.step()\n",
    "\n",
    "# show_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1235f-febd-457b-a5aa-190bc0220a0f",
   "metadata": {},
   "source": [
    "### Explanation of the next code cell\n",
    "\n",
    "First creates a ReLU (Rectified Linear Unit) function using PyTorch's built-in nn.ReLU() function.\n",
    "\n",
    "Next, it creates a tensor x with a value of -1.0 and sets requires_grad=True. This is important because it tells PyTorch that we want to calculate gradients with respect to x during the backward pass.\n",
    "\n",
    "Then, it applies the ReLU function to x and stores the result in y.\n",
    "\n",
    "The y.backward() function is then called to perform a backward pass through the computation graph. This calculates the gradient of y with respect to x.\n",
    "\n",
    "Finally, the gradient of x is accessed using x.grad and printed out. This is the gradient of the ReLU function at x.\n",
    "\n",
    "The ReLU function is defined as f(x) = max(0, x). So, its derivative (or gradient) is 1 for x > 0 and 0 for x <= 0. Since x is -1.0 in this case, the gradient at x is 0, which is what the code prints out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38a70975-ab20-41cc-8160-58e3564718b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# lecture 8 exercises\n",
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()  # calc's gradient of y wrt x\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad  # calculates the gradient at x\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab8998f2-cb96-454d-83f4-b54655119ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1000)\n"
     ]
    }
   ],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6386d3fd-f176-47e0-ac7b-43ca0ba3963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of parameters\n",
    "model = nn.Sequential(nn.Linear(16, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for parameter in model.parameters():\n",
    "  total += parameter.numel()\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4ce62547-6d05-49ae-95ce-332f302dbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02bfeb1e-a94f-4e46-89d0-392e3d1d63de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with exactly three linear layers and\n",
    "# less than 120 parameters, which takes n_features as inputs and outputs n_classes.\n",
    "n_features = 8\n",
    "n_classes = 2\n",
    "h1 = 6\n",
    "h2 = 6\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, h1),\n",
    "    nn.Linear(h1, h2),\n",
    "    nn.Linear(h2 ,n_classes)\n",
    ")\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e071bf4-e990-49b1-9cc1-29833ec60458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "# Create a neural network with exactly four linear layers and\n",
    "# more than 120 parameters, which takes n_features as inputs and outputs n_classes.\n",
    "n_features = 8\n",
    "n_classes = 2\n",
    "h = 6\n",
    "h1 = h\n",
    "h2 = h\n",
    "h3 = h\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model = model = nn.Sequential(\n",
    "    nn.Linear(n_features, h1),\n",
    "    nn.Linear(h1, h2),\n",
    "    nn.Linear(h2, h3),\n",
    "    nn.Linear(h3 ,n_classes)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c23b4-5333-4720-a145-b442bcd7040c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334c8f4-8e5e-4123-aea7-9242656dfb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d1650-5a92-46ee-948e-9ba5dd213515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
