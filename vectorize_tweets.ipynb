{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7613 labled tweets\n",
    "+ *test.csv* - 3236 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 6090 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1523 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f482e5-c7a0-4dfa-953a-f5b935dc0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\llmamd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57b58-2f5d-427a-8954-41f5a87b27db",
   "metadata": {},
   "source": [
    "## Vocabulary and tokenization\n",
    "\n",
    "### Empty string embedding\n",
    "\n",
    "After running all the text pre-processing steps (\"pipeline\"), some of the resulting tweets resulted in **empty strings**.  These result in **NaN** values when read in as dataframe and causes problems with `CountVectorize` which we need to build the token data matrix (rows = tweets, cols = token count in the tweet).\n",
    "\n",
    "There is an embedding for the empty string token in each of the `glove.twitter.27B...` embedding files at line 38523. Because there was no token to split on, the string \"<>\" was used as the token to represent the empty string so the `get_glove_embed` function could read this embedding properly.\n",
    "\n",
    "### Vectorizing a document using the entire input\n",
    "\n",
    "In this project, a tweet is considered a document.  Each word/token in the document is represented by a d-dimensional vector.  We can concatenate all these word vectors together to create one big vector.  For example, say we have a tweet:  *summer is lovely* and we are using 50d twitter glove embeddings, each word would be represented by the following vectors where ... are the values for the other 45 dimensions in the 50d vector:\n",
    "\n",
    "summer = [-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409]\n",
    "is = [0.18667 0.21368 0.14993, ..., -0.24608, -0.19549]\n",
    "lovely = [-0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "The entire tweet would then be represented by the following 150d vector:\n",
    "\n",
    "[-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409 | 0.18667 0.21368 0.14993, ..., -0.24608, -0.19549 | -0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "where the pipe character | was inserted after each word so it's easier to see.\n",
    "\n",
    "There are a couple of challenges to representing documents this way.  The first challenge is that our classification models need a fix input size.  The second challenge is that these vectors can get intractably large.\n",
    "\n",
    "\n",
    "### Vectorize a document with mean, min and max vectors\n",
    "\n",
    "A more common approach is to create a **mean** of the input embeddings and use this mean to represent the entire document.  Another related approach might be to create **min** and **max** vectors and concatenate them together to form a 2d dimensional vector where d = number of dimensions in the embedding vectors.  These min and max vectors are created from the minimum and maximum values of each dimension of the input embedding vectors respectively as described in the **Representing Document as Vectors** section of this workbook:\n",
    "\n",
    "https://github.com/MichaelSzczepaniak/WordEmbeddings/blob/master/WordEmbeddings.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8069eef8-89a2-4fee-9225-57137cd7df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "\n",
    "dict_glove_embs = pt.get_glove_embeds()  # default is glove.twitter.27B.200d.txt which takes ~1.3 min to load, ...50d.txt only take ~25 sec to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38459506-62a5-432f-a354-2367c3a549c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# <> inserted as empty string embedding token in twitter embedding files at line 38523\n",
    "# coeffs = \"-0.29736 -0.57305 -0.39627 0.11851 0.16625 0.20137 0.15891 0.27938 -0.078399 -0.12866 0.21086 0.10652 -0.45356 -0.60928 -0.44878 -0.10511 0.32838 -0.088057 0.051537 0.46852 -0.13936 -0.71007 -0.65363 0.23445 -0.19538 0.6608 0.1313 -0.045464 0.43522 -0.96466 0.18855 0.93414 0.68161 -0.64802 0.059672 -0.69549 -0.31669 -0.48399 -0.63895 -0.35644 0.14326 0.79823 0.41653 -0.10187 0.17715 -0.20817 -0.47895 0.36954 0.4828 0.37621 -0.3492 -0.089045 0.40169 -0.8378 0.19303 -0.16941 0.2664 0.49512 -0.20796 0.69913 0.43428 0.15835 0.38629 0.24039 0.031994 -0.14381 0.52596 0.28369 -0.27033 0.22807 0.23541 -0.39603 -0.31054 -0.78715 -0.71227 -0.029253 0.24174 -0.44296 -0.836 0.064297 -0.94075 -0.18824 -0.16903 0.5849 -0.0074337 0.626 -0.49226 -0.71578 0.35292 -0.21006 -0.24776 0.57754 -0.27919 0.70211 0.039619 0.34539 -0.14673 -0.81167 0.68231 0.52827 -0.52141 -0.69099 -0.75099 0.11661 0.98226 0.35352 -0.11707 0.45133 0.69767 0.19557 -0.364 -0.035521 -0.71357 -0.83975 0.20347 -0.039052 -0.63665 -0.4491 -0.16223 0.51879 -0.7832 0.0896 -0.037932 0.23763 -0.51888 -0.17253 -0.014441 -0.5044 0.26391 -0.53308 0.92899 0.043442 -0.17849 -0.24523 -0.45531 -0.069423 -0.21187 -0.41407 -0.090711 -0.34815 0.1754 -0.21396 -0.13499 -0.64721 -0.3795 -0.14429 -0.30074 0.61857 -0.065655 -0.14137 0.45494 0.26353 -1.1331 1.0426 -0.027096 0.23131 0.32532 -0.25335 -0.34065 0.28641 -0.25686 -1.1398 0.22298 -0.2051 -0.48052 -0.065082 -0.32023 -0.045533 0.093544 -0.28296 -0.34975 0.19851 0.0086796 0.12968 0.96043 0.4946 0.47144 -0.10981 0.67961 -0.42269 0.23401 0.38641 -0.18864 -0.8254 -0.098215 -0.27643 -0.17081 0.30223 -0.62112 -0.2338 -0.39195 -0.049065 -0.28386 0.24707 -0.13131 -0.33601 -0.92245 -0.32083 -0.28469 -0.43977\"\n",
    "# lst_coeffs = coeffs.split()\n",
    "# print(len(lst_coeffs))\n",
    "# vec_coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# print(vec_coeffs.shape, vec_coeffs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74243972-1bba-4b3b-8e84-7634ebfb4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 2 line commented out because empty string was manually added as <> token\n",
    "# coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# dict_glove_embs[''] = coeffs\n",
    "\n",
    "# find the nearest neighbor\n",
    "# pt.word_NN(\"<>\", dict_glove_embs, True)  # '\\x94', U+0094, Cancel Character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50491f7f-8981-4e91-9eba-f7f42d4c9795",
   "metadata": {},
   "source": [
    "### Checking token count distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7efd5e23-261e-4bd7-9590-603b8ba17ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import projtools as pt\n",
    "display = pd.options.display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_train_clean_v09 = pd.read_csv(\"./data/train_clean_v09.csv\", encoding=\"utf8\")\n",
    "df_aug_clean_v09 = pd.read_csv(\"./data/aug_clean_v09.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd5b66f-1833-460f-b61d-121bc881350a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>witness devastation cause powerful hurricane caribbean region pray safety all affect &lt;hashtag&gt; hurricane &lt;hashtag&gt; caribbean</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20004</td>\n",
       "      <td>devastate forest fire near la heart go all affect tragic disaster &lt;hashtag&gt; &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20005</td>\n",
       "      <td>break news authority local shelter place wildfire continue part california stay safe prepare &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20006</td>\n",
       "      <td>massive flooding force &lt;number&gt; resident evacuate rescue effort underway home water &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20007</td>\n",
       "      <td>receive video from &lt;hashtag&gt; california flood water engulf neighborhood stay safe everyone &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  20001   \n",
       "1  20004   \n",
       "2  20005   \n",
       "3  20006   \n",
       "4  20007   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "0  witness devastation cause powerful hurricane caribbean region pray safety all affect <hashtag> hurricane <hashtag> caribbean   \n",
       "1                                         devastate forest fire near la heart go all affect tragic disaster <hashtag> <hashtag>   \n",
       "2                        break news authority local shelter place wildfire continue part california stay safe prepare <hashtag>   \n",
       "3                                 massive flooding force <number> resident evacuate rescue effort underway home water <hashtag>   \n",
       "4                          receive video from <hashtag> california flood water engulf neighborhood stay safe everyone <hashtag>   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_aug_clean_v09.shape)\n",
    "df_aug_clean_v09.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bbd126-d74c-4f5f-9e6b-6eb5c4af9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# actual size of vocabulary\n",
    "# vocabulary_size = 4872\n",
    "\n",
    "## add the special tokens to token_pattern parameter so we can preserve them\n",
    "## <> added to fix issue with empty string and possibly use as padding\n",
    "vectorizer_v9 = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                                  token_pattern = r\"(?u)\\b\\w\\w+\\b|<user>|<hashtag>|<url>|<number>|<>\",\n",
    "                                  preprocessor = None, max_features = None)  #max_features = vocabulary_size)\n",
    "data_features_v09_train = vectorizer_v9.fit_transform(df_train_clean_v09['text'])\n",
    "## each row rep's a tweet, each column rep's a word in the vocabulary\n",
    "data_mat_v09_train = data_features_v09_train.toarray()  # each cell is the freqency of a word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d0fb3d-97d3-4a0e-b6d6-17fc8f6a56f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['deed',\n",
       "  'reason',\n",
       "  '<hashtag>',\n",
       "  'earthquake',\n",
       "  'may',\n",
       "  'allah',\n",
       "  'forgive',\n",
       "  'all',\n",
       "  'forest',\n",
       "  'fire'],\n",
       " 4819,\n",
       " (7485, 4819))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys are words in the vocabulary, each value is the column\n",
    "# in the data matrix representing a word (key) in the vocabulary\n",
    "voc_dict = vectorizer_v9.vocabulary_\n",
    "vocab = list(voc_dict.keys())\n",
    "vocab[0:10], len(vocab), data_mat_v09_train.shape  # why is |V| = 4819 and not 4872???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "976b43de-a843-4dc3-9db4-ca944edaf2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3474, 1, 4, 3, 1324)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict['reason'], voc_dict['<hashtag>'], voc_dict['<user>'], voc_dict['<url>'], voc_dict['earthquake']  # (3473, 0, 3, 2, 1323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f9b998-d749-4f7a-94d9-d2106e03e7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reason = data_mat_v09_train[:, 3473]  # 31 tweets have the word 'reason' in it - verified in NP++\n",
    "vec_reason.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a592a7c0-8d77-4418-a51b-b32bc68cd8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_tweet_train type: <class 'numpy.ndarray'>, shape: (7485,)\n",
      "minimum tokens per original tweet: 1\n",
      "maximum tokens per original tweet: 29\n"
     ]
    }
   ],
   "source": [
    "words_per_tweet_train = data_mat_v09_train.sum(axis=1)\n",
    "print(f\"words_per_tweet_train type: {type(words_per_tweet_train)}, shape: {words_per_tweet_train.shape}\")\n",
    "print(f\"minimum tokens per original tweet: {words_per_tweet_train.min()}\")\n",
    "print(f\"maximum tokens per original tweet: {words_per_tweet_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9855a-3c04-488c-88c1-656cd2a3a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which train tweets have 0 tokens? None after fix...\n",
    "# indices_with_0 = np.where(words_per_tweet_train == 0)[0]\n",
    "# id = 28, 36, 40, 6407, 7295, 8560 and 9919\n",
    "# indices_with_0  # array([  19,   24,   28, 4419, 5018, 5890, 6799], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0803b2-35ce-4047-886d-ae76e07a40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_v9_aug = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                                    token_pattern = r\"(?u)\\b\\w\\w+\\b|<user>|<hashtag>|<url>|<number>|<>\",\n",
    "                                    preprocessor = None, max_features = None)\n",
    "data_features_v09_aug = vectorizer_v9.fit_transform(df_aug_clean_v09['text'])\n",
    "## each row rep's an augmented tweet, each column rep's a word in the vocabulary\n",
    "data_mat_v09_aug = data_features_v09_aug.toarray()  # each cell is the freqency of a word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ce9788-5d60-4fb1-94a2-985af6830c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_tweet_aug type: <class 'numpy.ndarray'>, shape: (7485,)\n",
      "minimum tokens per augmented tweet: 1\n",
      "maximum tokens per augmented tweet: 26\n"
     ]
    }
   ],
   "source": [
    "words_per_tweet_aug = data_mat_v09_aug.sum(axis=1)\n",
    "print(f\"words_per_tweet_aug type: {type(words_per_tweet_train)}, shape: {words_per_tweet_aug.shape}\")\n",
    "print(f\"minimum tokens per augmented tweet: {words_per_tweet_aug.min()}\")\n",
    "print(f\"maximum tokens per augmented tweet: {words_per_tweet_aug.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c9529-d23e-4118-bbd1-a86bcf742af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de18c2-997f-4c05-a69a-0895b2e8241a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bad03-ef1d-46a7-ab06-c6f54d820d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88d04d2-7900-400b-baf6-940f2771638b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a31630-18d2-44bf-9040-b6d2c9b9769e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01864fd-367e-4a61-b599-64888f9116cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7cfe5-d702-4ceb-b818-8d349b99355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb30b39-795f-4711-800c-c3e651a10522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b6eed-2992-49b1-ba99-093817383f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac4bcb-fe7e-430c-88aa-5508efe0efb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861b050-0cd9-4ca8-b549-1f311142c81b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5855bb55-8174-420b-ada8-1ae0454cdcc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b183e-940b-4c01-bf1e-8ad0d4b165b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a6c83-060e-4af2-99eb-981375ea759f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c895e9-974b-489f-863b-8756c0db9a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0876e9-4843-4757-89c0-299883db8947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a23d7-0f71-494c-a502-4e16e333061d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38959db4-2d00-4123-8fe8-d6f81d657392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98571b2-971b-43a7-9044-8d040d56597c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac13ec8-668e-4c6a-bbf5-95fbe781fbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9a1f2-983c-49fb-86b3-5b9445770baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51964ba7-0960-4caf-a97b-bb84eda7d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8e75e-7826-4df7-b12c-c1070fa7c7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1f579-2c09-4564-b24c-d67c10480c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff714859-ddbd-410c-85fc-de047551abe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91b27-26e9-46d9-8079-689e9d238a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe11bb5-6846-46da-a716-eeea470742ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a5f6-db24-439f-a5c6-be4329d6e5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696941-a04d-4f3f-aeec-e133153a1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36480-1825-4f2a-8237-a9b1a73ff208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42422a-cfdf-4886-8076-ca74845a23f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025200d-3d63-402c-9545-7d93cbc76e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b72f0-3763-4933-962a-89048aa38d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7641e-4723-4cf3-b5a3-87377bc38c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
