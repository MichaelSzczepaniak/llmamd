{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7485 labled tweets **after duplicate removals** \n",
    "+ *test.csv* - 3263 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 5988 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1497 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Simplier NLP Classifier Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f482e5-c7a0-4dfa-953a-f5b935dc0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\llmamd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57b58-2f5d-427a-8954-41f5a87b27db",
   "metadata": {},
   "source": [
    "## Vocabulary and tokenization\n",
    "\n",
    "### Empty string embedding\n",
    "\n",
    "After running all the text pre-processing steps (\"pipeline\"), some of the resulting tweets resulted in **empty strings**.  These result in **NaN** values when read in as dataframe and causes problems with `CountVectorize` which we need to build the token data matrix (rows = tweets, cols = token count in the tweet).\n",
    "\n",
    "There is an embedding for the empty string token in each of the `glove.twitter.27B...` embedding files at line 38523. Because there was no token to split on, the string \"<>\" was used as the token to represent the empty string so the `get_glove_embed` function could read this embedding properly.\n",
    "\n",
    "### Vectorizing a document using the entire input\n",
    "\n",
    "In this project, a tweet is considered a document.  Each word/token in the document is represented by a d-dimensional vector.  We can concatenate all these word vectors together to create one big vector.  For example, say we have a tweet:  *summer is lovely* and we are using 50d twitter glove embeddings, each word would be represented by the following vectors where ... are the values for the other 45 dimensions in the 50d vector:\n",
    "\n",
    "summer = [-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409]\n",
    "is = [0.18667 0.21368 0.14993, ..., -0.24608, -0.19549]\n",
    "lovely = [-0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "The entire tweet would then be represented by the following 150d vector:\n",
    "\n",
    "[-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409 | 0.18667 0.21368 0.14993, ..., -0.24608, -0.19549 | -0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "where the pipe character | was inserted after each word so it's easier to see.\n",
    "\n",
    "There are a couple of challenges to representing documents this way.  The first challenge is that our classification models need a fix input size.  The second challenge is that these vectors can get intractably large.\n",
    "\n",
    "\n",
    "### Vectorize a document with mean, min and max vectors\n",
    "\n",
    "A more common approach is to create a **mean** of the input embeddings and use this mean to represent the entire document.  Another related approach might be to create **min** and **max** vectors and concatenate them together to form a 2d dimensional vector where d = number of dimensions in the embedding vectors.  These min and max vectors are created from the minimum and maximum values of each dimension of the input embedding vectors respectively as described in the **Representing Document as Vectors** section of this workbook:\n",
    "\n",
    "https://github.com/MichaelSzczepaniak/WordEmbeddings/blob/master/WordEmbeddings.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8069eef8-89a2-4fee-9225-57137cd7df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors...\n",
      "Found 1193514 word vectors\n",
      "Retrieving embeddings took  0.35 minutes\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dict_glove_embs = pt.get_glove_embeds()  # default is glove.twitter.27B.50d.txt which takes ~21 sec to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9538d32a-469b-4c50-85ff-ae330577b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict_glove_embs[\"<>\"].dtype)\n",
    "# print(dict_glove_embs[\"man\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38459506-62a5-432f-a354-2367c3a549c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <> inserted as empty string embedding token in twitter embedding files at line 38523\n",
    "# coeffs = \"-0.29736 -0.57305 -0.39627 0.11851 0.16625 0.20137 0.15891 0.27938 -0.078399 -0.12866 0.21086 0.10652 -0.45356 -0.60928 -0.44878 -0.10511 0.32838 -0.088057 0.051537 0.46852 -0.13936 -0.71007 -0.65363 0.23445 -0.19538 0.6608 0.1313 -0.045464 0.43522 -0.96466 0.18855 0.93414 0.68161 -0.64802 0.059672 -0.69549 -0.31669 -0.48399 -0.63895 -0.35644 0.14326 0.79823 0.41653 -0.10187 0.17715 -0.20817 -0.47895 0.36954 0.4828 0.37621 -0.3492 -0.089045 0.40169 -0.8378 0.19303 -0.16941 0.2664 0.49512 -0.20796 0.69913 0.43428 0.15835 0.38629 0.24039 0.031994 -0.14381 0.52596 0.28369 -0.27033 0.22807 0.23541 -0.39603 -0.31054 -0.78715 -0.71227 -0.029253 0.24174 -0.44296 -0.836 0.064297 -0.94075 -0.18824 -0.16903 0.5849 -0.0074337 0.626 -0.49226 -0.71578 0.35292 -0.21006 -0.24776 0.57754 -0.27919 0.70211 0.039619 0.34539 -0.14673 -0.81167 0.68231 0.52827 -0.52141 -0.69099 -0.75099 0.11661 0.98226 0.35352 -0.11707 0.45133 0.69767 0.19557 -0.364 -0.035521 -0.71357 -0.83975 0.20347 -0.039052 -0.63665 -0.4491 -0.16223 0.51879 -0.7832 0.0896 -0.037932 0.23763 -0.51888 -0.17253 -0.014441 -0.5044 0.26391 -0.53308 0.92899 0.043442 -0.17849 -0.24523 -0.45531 -0.069423 -0.21187 -0.41407 -0.090711 -0.34815 0.1754 -0.21396 -0.13499 -0.64721 -0.3795 -0.14429 -0.30074 0.61857 -0.065655 -0.14137 0.45494 0.26353 -1.1331 1.0426 -0.027096 0.23131 0.32532 -0.25335 -0.34065 0.28641 -0.25686 -1.1398 0.22298 -0.2051 -0.48052 -0.065082 -0.32023 -0.045533 0.093544 -0.28296 -0.34975 0.19851 0.0086796 0.12968 0.96043 0.4946 0.47144 -0.10981 0.67961 -0.42269 0.23401 0.38641 -0.18864 -0.8254 -0.098215 -0.27643 -0.17081 0.30223 -0.62112 -0.2338 -0.39195 -0.049065 -0.28386 0.24707 -0.13131 -0.33601 -0.92245 -0.32083 -0.28469 -0.43977\"\n",
    "# lst_coeffs = coeffs.split()\n",
    "# print(len(lst_coeffs))\n",
    "# vec_coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# print(vec_coeffs.shape, vec_coeffs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74243972-1bba-4b3b-8e84-7634ebfb4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 2 line commented out because empty string was manually added as <> token\n",
    "# coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# dict_glove_embs[''] = coeffs\n",
    "\n",
    "# find the nearest neighbor\n",
    "# pt.word_NN(\"<>\", dict_glove_embs, True)  # '\\x94', U+0094, Cancel Character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb34969-b98e-4965-81ab-52f072967461",
   "metadata": {},
   "source": [
    "## Tokens per tweet distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a266ac4c-cecd-449b-a701-f365256fa1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 4819)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# actual size of vocabulary\n",
    "# vocabulary_size = 4872\n",
    "\n",
    "## add the special tokens to token_pattern parameter so we can preserve them\n",
    "## <> added to fix issue with empty string and possibly use as padding\n",
    "vectorizer_v9 = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                                token_pattern = r\"(?u)\\b\\w\\w+\\b|<user>|<hashtag>|<url>|<number>|<>\",\n",
    "                                preprocessor = None, max_features = None)  #max_features = vocabulary_size)\n",
    "df_train_clean_v09 = pd.read_csv(\"./data/train_clean_v09.csv\", encoding=\"utf8\")\n",
    "data_features_v09_train = vectorizer_v9.fit_transform(df_train_clean_v09['text'])\n",
    "## each row rep's a tweet, each column rep's a word in the vocabulary\n",
    "data_mat_v09_train = data_features_v09_train.toarray()  # each cell is the freqency of a word in a tweet\n",
    "print(data_mat_v09_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed486916-2be4-4d02-a4c7-166fd6eedf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['deed',\n",
       "  'reason',\n",
       "  '<hashtag>',\n",
       "  'earthquake',\n",
       "  'may',\n",
       "  'allah',\n",
       "  'forgive',\n",
       "  'all',\n",
       "  'forest',\n",
       "  'fire'],\n",
       " 4819,\n",
       " (7485, 4819))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys are words in the vocabulary, each value is the column index\n",
    "# in the data matrix representing a word (key) in the vocabulary\n",
    "voc_dict = vectorizer_v9.vocabulary_\n",
    "vocab = list(voc_dict.keys())\n",
    "vocab[0:10], len(vocab), data_mat_v09_train.shape  # why is |V| = 4819 and not 4872? probably due to stop word removals..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80678dfb-0770-41f5-a87a-b61ca52dbe82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3474, 1, 4, 3, 1324)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict['reason'], voc_dict['<hashtag>'], voc_dict['<user>'], voc_dict['<url>'], voc_dict['earthquake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086cd875-3be7-4c12-aefc-6647898ee2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reason = data_mat_v09_train[:, voc_dict['reason']]  # 31 tweets have the word 'reason' in it - verified in NP++\n",
    "vec_reason.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaea4a3b-1d16-49b7-b392-9326ca65d78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_tweet_train type: <class 'numpy.ndarray'>, shape: (7485,)\n",
      "minimum tokens per original tweet: 1\n",
      "maximum tokens per original tweet: 29\n"
     ]
    }
   ],
   "source": [
    "words_per_tweet_train = data_mat_v09_train.sum(axis=1)\n",
    "print(f\"words_per_tweet_train type: {type(words_per_tweet_train)}, shape: {words_per_tweet_train.shape}\")\n",
    "print(f\"minimum tokens per original tweet: {words_per_tweet_train.min()}\")\n",
    "print(f\"maximum tokens per original tweet: {words_per_tweet_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c793a53-b107-416e-9896-969e93527d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34f5ed-f3b4-4d6a-9d45-74765966046f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ab98c-934b-4a3c-8a87-a97cce7aa76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50491f7f-8981-4e91-9eba-f7f42d4c9795",
   "metadata": {},
   "source": [
    "## Vectorize with all cleaned tweet tokens\n",
    "\n",
    "Since the max number of tokens in the cleaned original training data is 29 and 26 for the cleaned augmented data, a 30 token input will be selected.  This will give us an input to the model that is 30 (tokens / tweet) x (50 dimensions / token) = 1500 dimensions / tweet.\n",
    "\n",
    "Since all tweets will be less than 30 tokens, each input will be padded with the empty string token (<>).\n",
    "\n",
    "## Build feature matrices\n",
    "\n",
    "The following 4 feature matrices are built and exported so the can be read back in during modeling:\n",
    "\n",
    "+ **feats_matrix_aug.txt** - 7485 rows where each row is a vectorized tweet padded to 30 tokens where each token is represented by a 50d GloVe twitter embedding and the empty string is used as the padding token.  1500 cols are the tweets padded to 30 tokens which are each converted to a 50d GloVe embedding\n",
    "+ **feats_matrix_train_train.txt** - 80% of the original training data used to train each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_train_test.txt** - 20% of the original training data used to test each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_test.txt** - unlabeled test data provided by kaggle to test submissions, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "\n",
    "The first 3 feature matrices have the following corresponding labels (`feats_matrix_test.txt` are unlabeled tweets):\n",
    "+ labels_aug.txt\n",
    "+ labels_train_train.txt\n",
    "+ labels_train_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efd5e23-261e-4bd7-9590-603b8ba17ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = pd.options.display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# df_train_clean_v09 = pd.read_csv(\"./data/train_clean_v09.csv\", encoding=\"utf8\")  # read in at cell xx\n",
    "train_clean_vec_text = df_train_clean_v09['text']\n",
    "train_clean_vec_target = df_train_clean_v09['target']\n",
    "train_targets = np.array(train_clean_vec_target)\n",
    "\n",
    "df_train_clean_v09_class0 = df_train_clean_v09.loc[df_train_clean_v09['target'] == 0, :]\n",
    "df_train_clean_v09_class1 = df_train_clean_v09.loc[df_train_clean_v09['target'] == 1, :]\n",
    "df_aug_clean_v09 = pd.read_csv(\"./data/aug_clean_v09.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd5b66f-1833-460f-b61d-121bc881350a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of df_aug_clean_v09 (7485, 3)\n",
      "Building tweet feature matrix took  0.99 minutes\n",
      "shape of feats_matrix_aug: (7485, 1500), dtype of feats_matrix_aug: float64\n",
      "shape of labels_aug: (7485,), dtype of labels_aug: int64\n"
     ]
    }
   ],
   "source": [
    "# start with feats_matrix_aug.txt and labels_aug.txt\n",
    "print(f\"shape of df_aug_clean_v09 {df_aug_clean_v09.shape}\")\n",
    "# df_aug_clean_v09.head()\n",
    "\n",
    "aug_clean_vec_id = df_aug_clean_v09['id']\n",
    "aug_clean_vec_text = df_aug_clean_v09['text']\n",
    "aug_clean_vec_target = df_aug_clean_v09['target']\n",
    "# vectorize aug padded to 30 tokens\n",
    "aug_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in aug_clean_vec_text]\n",
    "# build the feature matrix from the list of numpy arrays\n",
    "feats_matrix_aug = pt.make_tweet_feats(aug_clean_vec_text_vector_pad30)  # takes ~1min, 20sec to run\n",
    "labels_aug = np.array(aug_clean_vec_target)\n",
    "print(f\"shape of feats_matrix_aug: {feats_matrix_aug.shape}, dtype of feats_matrix_aug: {feats_matrix_aug.dtype}\")\n",
    "print(f\"shape of labels_aug: {labels_aug.shape}, dtype of labels_aug: {labels_aug.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980519b2-9573-4bcf-ad0d-bb94f2ba6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feat_matrix_aug and labels_aug as a text files\n",
    "np.savetxt(fname='./data/feats_matrix_aug.txt', X=feats_matrix_aug, fmt='%9.6f')  # use np.loadtxt to bring this back in\n",
    "labels_aug = np.array(aug_clean_vec_target)\n",
    "np.savetxt(fname='./data/labels_aug.txt', X=labels_aug, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d05f1a-f3b9-4903-838f-12d6ed0b8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5988 samples from the train set will be used to train the model\n",
      "1497 samples from the train set will be used to test the model\n"
     ]
    }
   ],
   "source": [
    "# work on feats_matrix_train_train.txt, start by splitting training into train_train\n",
    "# and train_test because test data is unlabeled\n",
    "np.random.seed(711)\n",
    "portion_test = 0.20  # 80/20 train/test split\n",
    "# compute the number of training and testing samples\n",
    "train_train_samples = int((1. - portion_test) * train_targets.shape[0])\n",
    "train_test_samples = train_targets.shape[0] - train_train_samples\n",
    "print(f\"{train_train_samples} samples from the train set will be used to train the model\")\n",
    "print(f\"{train_test_samples} samples from the train set will be used to test the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "597cf1fa-ad3a-4577-9c67-eb20373f02cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tweet feature matrix took  0.99 minutes\n",
      "(7485, 1500) float64\n"
     ]
    }
   ],
   "source": [
    "train_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in train_clean_vec_text]\n",
    "feats_matrix_train = pt.make_tweet_feats(train_clean_vec_text_vector_pad30)  # all train features: train_train & train_test\n",
    "print(feats_matrix_train.shape, feats_matrix_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c770b01-ab05-421b-80c2-cf124122269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 class 0 train test samples, 749 class 1 train test samples\n"
     ]
    }
   ],
   "source": [
    "# calc the number of samples from each class used to test\n",
    "portion_class0_test = 0.5\n",
    "class0_test_samples = int(train_test_samples * portion_class0_test)\n",
    "class1_test_samples = train_test_samples - class0_test_samples\n",
    "print(f\"{class0_test_samples} class 0 train test samples, {class1_test_samples} class 1 train test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfdc7170-2a28-4ff4-b9d5-16dfdce61fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the train_train_train_data: (5988, 1500), shape of the train_train_labels: (5988,)\n"
     ]
    }
   ],
   "source": [
    "# compute indices for 748 class 0 and 749 class 1 random samples,\n",
    "# need the [0] on the np.where calls because it returns a 2-tuple\n",
    "# and the results of the 1st logical condition are all we want\n",
    "train_test_inds = np.append(np.random.choice((np.where(train_targets==0))[0], class0_test_samples, replace=False),\n",
    "                            np.random.choice((np.where(train_targets==1))[0], class1_test_samples, replace=False))\n",
    "# compute training set indices from indices not in the test set\n",
    "train_train_inds = list(set(range(len(train_targets))) - set(train_test_inds))\n",
    "# original training data used to train model\n",
    "feats_matrix_train_train = feats_matrix_train[train_train_inds, ]\n",
    "labels_train_train = train_targets[train_train_inds, ]\n",
    "print(f\"shape of the train_train_train_data: {feats_matrix_train_train.shape}, shape of the train_train_labels: {labels_train_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c7d847-e339-492e-a0c2-549b4678cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the train_train_data and train_train_labels\n",
    "np.savetxt(fname='./data/feats_matrix_train_train.txt', X=feats_matrix_train_train, fmt='%9.6f')\n",
    "np.savetxt(fname='./data/labels_train_train.txt', X=labels_train_train, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a430bbf3-0790-4a44-bf4e-da99cb09ecb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the train_test_train_data: (1497, 1500), shape of the train_test_labels: (1497,)\n"
     ]
    }
   ],
   "source": [
    "# work on feats_matrix_train_test.txt and labels_train_test.txt\n",
    "feats_matrix_train_test = feats_matrix_train[train_test_inds, ]\n",
    "labels_train_test = train_targets[train_test_inds, ]\n",
    "print(f\"shape of the train_test_train_data: {feats_matrix_train_test.shape}, shape of the train_test_labels: {labels_train_test.shape}\")\n",
    "# write them out\n",
    "np.savetxt(fname='./data/feats_matrix_train_test.txt', X=feats_matrix_train_test, fmt='%9.6f')\n",
    "np.savetxt(fname='./data/labels_train_test.txt', X=labels_train_test, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c970306-903a-45ec-af7c-18ba5cc2e4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building tweet feature matrix took  0.19 minutes\n",
      "shape of the feats_matrix_test: (3263, 1500)\n"
     ]
    }
   ],
   "source": [
    "# build and write the feature matrix for the unlabeled test data\n",
    "df_test_clean_v09 = pd.read_csv(\"./data/test_clean_v09.csv\", encoding=\"utf8\")\n",
    "test_clean_vec_text = df_test_clean_v09['text']\n",
    "test_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in test_clean_vec_text]\n",
    "feats_matrix_test = pt.make_tweet_feats(test_clean_vec_text_vector_pad30)\n",
    "print(f\"shape of the feats_matrix_test: {feats_matrix_test.shape}\")\n",
    "np.savetxt(fname='./data/feats_matrix_test.txt', X=feats_matrix_test, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5c34184-d8aa-44c6-be99-e0a2b99a99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of computed feats_matrix_aug: (7485, 1500), shape of read in feats_matrix_aug: (7485, 1500)\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# read in the data and check that it's similar to what we wrote out\n",
    "feats_matrix_aug_ff = np.loadtxt('./data/feats_matrix_aug.txt')\n",
    "feats_matrix_train_train_ff = np.loadtxt('./data/feats_matrix_train_train.txt')\n",
    "feats_matrix_train_test_ff = np.loadtxt('./data/feats_matrix_train_test.txt')\n",
    "feats_matrix_test_ff = np.loadtxt('./data/feats_matrix_test.txt')\n",
    "# test the load\n",
    "print(f\"shape of computed feats_matrix_aug: {feats_matrix_aug.shape}, shape of read in feats_matrix_aug: {feats_matrix_aug_ff.shape}\")\n",
    "print(np.allclose(feats_matrix_aug, feats_matrix_aug_ff, rtol=1e-04, atol=1e-06))\n",
    "# print(np.allclose(feats_matrix_aug, feats_matrix_aug_ff))  # not sure why there is this large a difference...\n",
    "print(np.allclose(feats_matrix_train_train, feats_matrix_train_train_ff, rtol=1e-04, atol=1e-06))\n",
    "print(np.allclose(feats_matrix_train_test, feats_matrix_train_test_ff, rtol=1e-04, atol=1e-06))\n",
    "print(np.allclose(feats_matrix_test, feats_matrix_test_ff, rtol=1e-04, atol=1e-06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
