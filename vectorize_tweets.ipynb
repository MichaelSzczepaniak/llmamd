{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7485 labled tweets **after duplicate removals** \n",
    "+ *test.csv* - 3263 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 6090 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1523 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Non-Transformer Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f482e5-c7a0-4dfa-953a-f5b935dc0ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\llmamd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b57b58-2f5d-427a-8954-41f5a87b27db",
   "metadata": {},
   "source": [
    "## Vocabulary and tokenization\n",
    "\n",
    "### Empty string embedding\n",
    "\n",
    "After running all the text pre-processing steps (\"pipeline\"), some of the resulting tweets resulted in **empty strings**.  These result in **NaN** values when read in as dataframe and causes problems with `CountVectorize` which we need to build the token data matrix (rows = tweets, cols = token count in the tweet).\n",
    "\n",
    "There is an embedding for the empty string token in each of the `glove.twitter.27B...` embedding files at line 38523. Because there was no token to split on, the string \"<>\" was used as the token to represent the empty string so the `get_glove_embed` function could read this embedding properly.\n",
    "\n",
    "### Vectorizing a document using the entire input\n",
    "\n",
    "In this project, a tweet is considered a document.  Each word/token in the document is represented by a d-dimensional vector.  We can concatenate all these word vectors together to create one big vector.  For example, say we have a tweet:  *summer is lovely* and we are using 50d twitter glove embeddings, each word would be represented by the following vectors where ... are the values for the other 45 dimensions in the 50d vector:\n",
    "\n",
    "summer = [-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409]\n",
    "is = [0.18667 0.21368 0.14993, ..., -0.24608, -0.19549]\n",
    "lovely = [-0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "The entire tweet would then be represented by the following 150d vector:\n",
    "\n",
    "[-0.40501, -0.56994, 0.34398, ..., -0.95337, 1.1409 | 0.18667 0.21368 0.14993, ..., -0.24608, -0.19549 | -0.27926 -0.16338 0.50486, ..., -0.15416, -0.20196]\n",
    "\n",
    "where the pipe character | was inserted after each word so it's easier to see.\n",
    "\n",
    "There are a couple of challenges to representing documents this way.  The first challenge is that our classification models need a fix input size.  The second challenge is that these vectors can get intractably large.\n",
    "\n",
    "\n",
    "### Vectorize a document with mean, min and max vectors\n",
    "\n",
    "A more common approach is to create a **mean** of the input embeddings and use this mean to represent the entire document.  Another related approach might be to create **min** and **max** vectors and concatenate them together to form a 2d dimensional vector where d = number of dimensions in the embedding vectors.  These min and max vectors are created from the minimum and maximum values of each dimension of the input embedding vectors respectively as described in the **Representing Document as Vectors** section of this workbook:\n",
    "\n",
    "https://github.com/MichaelSzczepaniak/WordEmbeddings/blob/master/WordEmbeddings.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8069eef8-89a2-4fee-9225-57137cd7df52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import projtools as pt\n",
    "\n",
    "dict_glove_embs = pt.get_glove_embeds()  # default is glove.twitter.27B.200d.txt which takes ~1.3 min to load, ...50d.txt only take ~25 sec to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9538d32a-469b-4c50-85ff-ae330577b67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(dict_glove_embs[\"<>\"].dtype)\n",
    "print(dict_glove_embs[\"man\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38459506-62a5-432f-a354-2367c3a549c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# <> inserted as empty string embedding token in twitter embedding files at line 38523\n",
    "# coeffs = \"-0.29736 -0.57305 -0.39627 0.11851 0.16625 0.20137 0.15891 0.27938 -0.078399 -0.12866 0.21086 0.10652 -0.45356 -0.60928 -0.44878 -0.10511 0.32838 -0.088057 0.051537 0.46852 -0.13936 -0.71007 -0.65363 0.23445 -0.19538 0.6608 0.1313 -0.045464 0.43522 -0.96466 0.18855 0.93414 0.68161 -0.64802 0.059672 -0.69549 -0.31669 -0.48399 -0.63895 -0.35644 0.14326 0.79823 0.41653 -0.10187 0.17715 -0.20817 -0.47895 0.36954 0.4828 0.37621 -0.3492 -0.089045 0.40169 -0.8378 0.19303 -0.16941 0.2664 0.49512 -0.20796 0.69913 0.43428 0.15835 0.38629 0.24039 0.031994 -0.14381 0.52596 0.28369 -0.27033 0.22807 0.23541 -0.39603 -0.31054 -0.78715 -0.71227 -0.029253 0.24174 -0.44296 -0.836 0.064297 -0.94075 -0.18824 -0.16903 0.5849 -0.0074337 0.626 -0.49226 -0.71578 0.35292 -0.21006 -0.24776 0.57754 -0.27919 0.70211 0.039619 0.34539 -0.14673 -0.81167 0.68231 0.52827 -0.52141 -0.69099 -0.75099 0.11661 0.98226 0.35352 -0.11707 0.45133 0.69767 0.19557 -0.364 -0.035521 -0.71357 -0.83975 0.20347 -0.039052 -0.63665 -0.4491 -0.16223 0.51879 -0.7832 0.0896 -0.037932 0.23763 -0.51888 -0.17253 -0.014441 -0.5044 0.26391 -0.53308 0.92899 0.043442 -0.17849 -0.24523 -0.45531 -0.069423 -0.21187 -0.41407 -0.090711 -0.34815 0.1754 -0.21396 -0.13499 -0.64721 -0.3795 -0.14429 -0.30074 0.61857 -0.065655 -0.14137 0.45494 0.26353 -1.1331 1.0426 -0.027096 0.23131 0.32532 -0.25335 -0.34065 0.28641 -0.25686 -1.1398 0.22298 -0.2051 -0.48052 -0.065082 -0.32023 -0.045533 0.093544 -0.28296 -0.34975 0.19851 0.0086796 0.12968 0.96043 0.4946 0.47144 -0.10981 0.67961 -0.42269 0.23401 0.38641 -0.18864 -0.8254 -0.098215 -0.27643 -0.17081 0.30223 -0.62112 -0.2338 -0.39195 -0.049065 -0.28386 0.24707 -0.13131 -0.33601 -0.92245 -0.32083 -0.28469 -0.43977\"\n",
    "# lst_coeffs = coeffs.split()\n",
    "# print(len(lst_coeffs))\n",
    "# vec_coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# print(vec_coeffs.shape, vec_coeffs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74243972-1bba-4b3b-8e84-7634ebfb4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 2 line commented out because empty string was manually added as <> token\n",
    "# coeffs = np.fromstring(coeffs, dtype='float', sep=' ')\n",
    "# dict_glove_embs[''] = coeffs\n",
    "\n",
    "# find the nearest neighbor\n",
    "# pt.word_NN(\"<>\", dict_glove_embs, True)  # '\\x94', U+0094, Cancel Character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50491f7f-8981-4e91-9eba-f7f42d4c9795",
   "metadata": {},
   "source": [
    "### Checking token count distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7efd5e23-261e-4bd7-9590-603b8ba17ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import projtools as pt\n",
    "display = pd.options.display\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df_train_clean_v09 = pd.read_csv(\"./data/train_clean_v09.csv\", encoding=\"utf8\")\n",
    "df_aug_clean_v09 = pd.read_csv(\"./data/aug_clean_v09.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd5b66f-1833-460f-b61d-121bc881350a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001</td>\n",
       "      <td>witness devastation cause powerful hurricane caribbean region pray safety all affect &lt;hashtag&gt; hurricane &lt;hashtag&gt; caribbean</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20004</td>\n",
       "      <td>devastate forest fire near la heart go all affect tragic disaster &lt;hashtag&gt; &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20005</td>\n",
       "      <td>break news authority local shelter place wildfire continue part california stay safe prepare &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20006</td>\n",
       "      <td>massive flooding force &lt;number&gt; resident evacuate rescue effort underway home water &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20007</td>\n",
       "      <td>receive video from &lt;hashtag&gt; california flood water engulf neighborhood stay safe everyone &lt;hashtag&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  20001   \n",
       "1  20004   \n",
       "2  20005   \n",
       "3  20006   \n",
       "4  20007   \n",
       "\n",
       "                                                                                                                           text  \\\n",
       "0  witness devastation cause powerful hurricane caribbean region pray safety all affect <hashtag> hurricane <hashtag> caribbean   \n",
       "1                                         devastate forest fire near la heart go all affect tragic disaster <hashtag> <hashtag>   \n",
       "2                        break news authority local shelter place wildfire continue part california stay safe prepare <hashtag>   \n",
       "3                                 massive flooding force <number> resident evacuate rescue effort underway home water <hashtag>   \n",
       "4                          receive video from <hashtag> california flood water engulf neighborhood stay safe everyone <hashtag>   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_aug_clean_v09.shape)\n",
    "df_aug_clean_v09.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45bbd126-d74c-4f5f-9e6b-6eb5c4af9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# actual size of vocabulary\n",
    "# vocabulary_size = 4872\n",
    "\n",
    "## add the special tokens to token_pattern parameter so we can preserve them\n",
    "## <> added to fix issue with empty string and possibly use as padding\n",
    "vectorizer_v9 = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                                  token_pattern = r\"(?u)\\b\\w\\w+\\b|<user>|<hashtag>|<url>|<number>|<>\",\n",
    "                                  preprocessor = None, max_features = None)  #max_features = vocabulary_size)\n",
    "data_features_v09_train = vectorizer_v9.fit_transform(df_train_clean_v09['text'])\n",
    "## each row rep's a tweet, each column rep's a word in the vocabulary\n",
    "data_mat_v09_train = data_features_v09_train.toarray()  # each cell is the freqency of a word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d0fb3d-97d3-4a0e-b6d6-17fc8f6a56f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['deed',\n",
       "  'reason',\n",
       "  '<hashtag>',\n",
       "  'earthquake',\n",
       "  'may',\n",
       "  'allah',\n",
       "  'forgive',\n",
       "  'all',\n",
       "  'forest',\n",
       "  'fire'],\n",
       " 4819,\n",
       " (7485, 4819))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys are words in the vocabulary, each value is the column\n",
    "# in the data matrix representing a word (key) in the vocabulary\n",
    "voc_dict = vectorizer_v9.vocabulary_\n",
    "vocab = list(voc_dict.keys())\n",
    "vocab[0:10], len(vocab), data_mat_v09_train.shape  # why is |V| = 4819 and not 4872???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976b43de-a843-4dc3-9db4-ca944edaf2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3474, 1, 4, 3, 1324)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_dict['reason'], voc_dict['<hashtag>'], voc_dict['<user>'], voc_dict['<url>'], voc_dict['earthquake']  # (3473, 0, 3, 2, 1323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f9b998-d749-4f7a-94d9-d2106e03e7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_reason = data_mat_v09_train[:, 3473]  # 31 tweets have the word 'reason' in it - verified in NP++\n",
    "vec_reason.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a592a7c0-8d77-4418-a51b-b32bc68cd8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_tweet_train type: <class 'numpy.ndarray'>, shape: (7485,)\n",
      "minimum tokens per original tweet: 1\n",
      "maximum tokens per original tweet: 29\n"
     ]
    }
   ],
   "source": [
    "words_per_tweet_train = data_mat_v09_train.sum(axis=1)\n",
    "print(f\"words_per_tweet_train type: {type(words_per_tweet_train)}, shape: {words_per_tweet_train.shape}\")\n",
    "print(f\"minimum tokens per original tweet: {words_per_tweet_train.min()}\")\n",
    "print(f\"maximum tokens per original tweet: {words_per_tweet_train.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23c9855a-3c04-488c-88c1-656cd2a3a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which train tweets have 0 tokens? None after fix...\n",
    "# indices_with_0 = np.where(words_per_tweet_train == 0)[0]\n",
    "# id = 28, 36, 40, 6407, 7295, 8560 and 9919\n",
    "# indices_with_0  # array([  19,   24,   28, 4419, 5018, 5890, 6799], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e0803b2-35ce-4047-886d-ae76e07a40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_v9_aug = CountVectorizer(analyzer = \"word\", tokenizer = None,\n",
    "                                    token_pattern = r\"(?u)\\b\\w\\w+\\b|<user>|<hashtag>|<url>|<number>|<>\",\n",
    "                                    preprocessor = None, max_features = None)\n",
    "data_features_v09_aug = vectorizer_v9.fit_transform(df_aug_clean_v09['text'])\n",
    "## each row rep's an augmented tweet, each column rep's a word in the vocabulary\n",
    "data_mat_v09_aug = data_features_v09_aug.toarray()  # each cell is the freqency of a word in a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ce9788-5d60-4fb1-94a2-985af6830c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words_per_tweet_aug type: <class 'numpy.ndarray'>, shape: (7485,)\n",
      "minimum tokens per augmented tweet: 1\n",
      "maximum tokens per augmented tweet: 26\n"
     ]
    }
   ],
   "source": [
    "words_per_tweet_aug = data_mat_v09_aug.sum(axis=1)\n",
    "print(f\"words_per_tweet_aug type: {type(words_per_tweet_train)}, shape: {words_per_tweet_aug.shape}\")\n",
    "print(f\"minimum tokens per augmented tweet: {words_per_tweet_aug.min()}\")\n",
    "print(f\"maximum tokens per augmented tweet: {words_per_tweet_aug.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2ab6c-7eca-4140-9c15-8d9fae6db94a",
   "metadata": {},
   "source": [
    "## Vectorize with all cleaned tweet tokens\n",
    "\n",
    "Since the max number of token in the cleaned original training data is 29 and 26 for the cleaned augmented data, a 30 token input will be selected.  This will give us an input to the model that is 30 (tokens / tweet) x (50 dimensions / token) = 1500 dimensions / tweet.\n",
    "\n",
    "Since all tweets will be less than 30 tokens, each input will be padded with empty string token (<>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7de18c2-997f-4c05-a69a-0895b2e8241a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man: [ 0.454      0.091616  -0.013785  -0.82997   -0.96978    0.49015\n",
      "  0.31657    0.74712   -0.48305    0.61955   -0.29768    0.92822\n",
      " -4.4227     0.12139    0.31704    0.48146    0.40892    0.04774\n",
      " -0.40095   -1.1193    -0.32822    1.0673    -0.10623    0.28587\n",
      "  0.5852    -0.24609    0.20463   -0.8715    -0.47394   -0.80522\n",
      " -0.0020135  0.56237   -0.20017   -0.10347    0.80586   -0.031987\n",
      " -0.59742   -0.13249   -0.80784   -0.79229   -1.2715     0.46771\n",
      " -0.22299    1.0151    -0.51008    0.01278    0.96641    0.49574\n",
      "  0.13395    0.37403    0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064     0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of test_tweet vector: (250,)\n",
      "love fruit: [-0.20258    0.20543    0.17236    0.74813   -0.67589    0.15501\n",
      "  1.2159     0.076141  -0.54138    0.13545   -0.71692    0.3132\n",
      " -5.1367    -0.51263   -1.1023     0.2162    -0.17089   -0.11303\n",
      " -0.60699    0.62725    0.58585    0.37587   -0.22522    0.98157\n",
      "  0.32612   -0.064337  -0.082492   0.34397    1.0403    -1.0402\n",
      " -0.62527   -0.42018   -0.43558    0.58537    0.11988   -0.24759\n",
      "  0.44675   -0.097309  -0.13688    0.50034   -1.6759    -0.84282\n",
      "  0.18427    0.11367   -0.57527    0.12956    0.13157   -0.27141\n",
      "  0.17583   -0.30676    0.0031071 -0.80285   -0.77317    0.50469\n",
      "  0.89423    0.90219   -0.53821   -0.52817   -0.12731    0.0016473\n",
      " -0.66786    0.1926    -3.2961     0.34091   -0.47873    0.59462\n",
      "  1.0438    -0.22079    0.62558   -0.076996  -0.69564    0.085603\n",
      "  0.030264  -0.21562    0.53267    0.26031   -0.57479   -0.12539\n",
      "  0.35832   -0.98811    0.82293    0.35036   -1.0765    -0.13438\n",
      " -0.40119   -0.34154   -0.21204   -0.76575    0.81553    1.0711\n",
      " -0.1086    -0.90065   -1.4035    -0.54709    0.73617    0.29896\n",
      "  0.51953    0.33532    0.011005   0.75756    0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of test_tweet vector: (250,)\n",
      "<>: [ 0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064     0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of test_tweet vector: (250,)\n",
      "you like pasta: [ 0.051028   0.48649   -0.18347    0.33075   -0.47007   -0.069412\n",
      "  1.131      0.31791   -0.12472    0.82891   -0.35092    0.051084\n",
      " -6.4007    -0.80443   -0.85157    0.23292   -0.08702   -0.42304\n",
      " -0.26702   -0.003675   0.46597    0.13554    0.10425    0.9524\n",
      " -0.048529   0.24545    0.38401    0.0022835  1.191     -0.30374\n",
      " -0.44441    0.082608   0.23851    0.33268    0.1327     0.38742\n",
      "  0.54447    0.21171   -0.083061   0.2974    -1.0858    -0.36229\n",
      "  0.43826    0.30009   -0.074377  -0.38753   -0.22611    0.071517\n",
      "  0.13473   -0.0738    -0.094661   0.13413    0.21436    0.0081264\n",
      "  0.046816   0.049615   0.98244    0.64103   -0.0082674  0.68412\n",
      " -0.080717   0.15725   -5.6624    -0.60854   -0.60499   -0.31969\n",
      "  0.042107  -0.97399   -0.22257   -0.046089   0.053504   0.14328\n",
      "  0.32858    0.18924   -0.034724   0.04656   -0.15097    0.30726\n",
      "  0.11143   -0.55127   -0.62121    0.717      0.0066723  0.070774\n",
      " -0.15629    0.37838   -0.17075   -0.1874     0.016917   0.18845\n",
      " -1.1152    -0.15614   -0.10915   -0.20781    0.070383  -0.096904\n",
      "  0.55417    0.47062   -0.44501    0.13283   -0.1898    -0.48601\n",
      "  0.79041   -0.13132    1.0374     0.14877   -1.1636    -0.53053\n",
      "  0.1889     0.12433   -0.39479   -0.70221   -2.0926     0.54484\n",
      " -0.19711    0.18826    0.95606   -0.84976    0.054376   0.014243\n",
      " -0.70101   -0.0082734 -0.28095    0.58022    0.19792   -0.6023\n",
      " -0.070511  -0.13207    0.37856   -1.1142     0.20955    0.47169\n",
      " -0.74701   -0.71499   -0.6417     0.12979    0.77034    0.060835\n",
      " -0.39208    1.4978    -1.1134    -0.59952   -1.2542    -0.65079\n",
      "  1.4982     0.40878    0.87208    0.23229   -0.24253   -0.65425\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of test_tweet vector: (250,)\n"
     ]
    }
   ],
   "source": [
    "# some short tweets from train_clean_v09, id=23,24,40,41\n",
    "padded_token_count = 5\n",
    "test_tweets = [\"man\", \"love fruit\", \"<>\", \"you like pasta\"]\n",
    "tweet_vecs = {}\n",
    "pad_vec = dict_glove_embs[\"<>\"]\n",
    "for test_tweet in test_tweets:\n",
    "    tokens = test_tweet.split()\n",
    "    padding_tokens = padded_token_count - len(tokens)\n",
    "    token_vec = np.array([])\n",
    "    for token in tokens:\n",
    "        token_vec = np.hstack((token_vec, dict_glove_embs[token]))\n",
    "    # add the padding\n",
    "    for pad_token in range(padding_tokens):\n",
    "        token_vec = np.hstack((token_vec, pad_vec))\n",
    "    \n",
    "    tweet_vecs[test_tweet] = token_vec\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(f\"{test_tweet}: {tweet_vecs[test_tweet]}\")\n",
    "    print(f\"shape of test_tweet vector: {tweet_vecs[test_tweet].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a24bad03-ef1d-46a7-ab06-c6f54d820d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man: [ 0.454      0.091616  -0.013785  -0.82997   -0.96978    0.49015\n",
      "  0.31657    0.74712   -0.48305    0.61955   -0.29768    0.92822\n",
      " -4.4227     0.12139    0.31704    0.48146    0.40892    0.04774\n",
      " -0.40095   -1.1193    -0.32822    1.0673    -0.10623    0.28587\n",
      "  0.5852    -0.24609    0.20463   -0.8715    -0.47394   -0.80522\n",
      " -0.0020135  0.56237   -0.20017   -0.10347    0.80586   -0.031987\n",
      " -0.59742   -0.13249   -0.80784   -0.79229   -1.2715     0.46771\n",
      " -0.22299    1.0151    -0.51008    0.01278    0.96641    0.49574\n",
      "  0.13395    0.37403    0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064     0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of tweet vector: (250,)\n",
      "love fruit: [-0.20258    0.20543    0.17236    0.74813   -0.67589    0.15501\n",
      "  1.2159     0.076141  -0.54138    0.13545   -0.71692    0.3132\n",
      " -5.1367    -0.51263   -1.1023     0.2162    -0.17089   -0.11303\n",
      " -0.60699    0.62725    0.58585    0.37587   -0.22522    0.98157\n",
      "  0.32612   -0.064337  -0.082492   0.34397    1.0403    -1.0402\n",
      " -0.62527   -0.42018   -0.43558    0.58537    0.11988   -0.24759\n",
      "  0.44675   -0.097309  -0.13688    0.50034   -1.6759    -0.84282\n",
      "  0.18427    0.11367   -0.57527    0.12956    0.13157   -0.27141\n",
      "  0.17583   -0.30676    0.0031071 -0.80285   -0.77317    0.50469\n",
      "  0.89423    0.90219   -0.53821   -0.52817   -0.12731    0.0016473\n",
      " -0.66786    0.1926    -3.2961     0.34091   -0.47873    0.59462\n",
      "  1.0438    -0.22079    0.62558   -0.076996  -0.69564    0.085603\n",
      "  0.030264  -0.21562    0.53267    0.26031   -0.57479   -0.12539\n",
      "  0.35832   -0.98811    0.82293    0.35036   -1.0765    -0.13438\n",
      " -0.40119   -0.34154   -0.21204   -0.76575    0.81553    1.0711\n",
      " -0.1086    -0.90065   -1.4035    -0.54709    0.73617    0.29896\n",
      "  0.51953    0.33532    0.011005   0.75756    0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of tweet vector: (250,)\n",
      "<>: [ 0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064     0.45973   -0.16703\n",
      " -1.2028     0.41675    0.14643   -0.39861   -0.35118   -0.46944\n",
      "  0.63799    0.49569   -0.038122  -0.37854   -1.2221    -1.0439\n",
      " -1.2604     0.01232   -0.5159     0.1357    -0.093283   0.12307\n",
      "  0.48072   -0.66419    0.50046   -0.58255    0.81583    0.72197\n",
      " -0.101     -0.17283    0.51572    0.3296    -0.0024615  0.19475\n",
      "  2.1163     0.20636   -1.2026    -0.0767    -0.1058    -0.82518\n",
      " -0.31287   -0.19303    0.061489  -0.30422    0.75731   -0.53688\n",
      " -0.22277   -0.22173   -0.37943    0.17821   -0.34743    0.3064\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of tweet vector: (250,)\n",
      "you like pasta: [ 0.051028   0.48649   -0.18347    0.33075   -0.47007   -0.069412\n",
      "  1.131      0.31791   -0.12472    0.82891   -0.35092    0.051084\n",
      " -6.4007    -0.80443   -0.85157    0.23292   -0.08702   -0.42304\n",
      " -0.26702   -0.003675   0.46597    0.13554    0.10425    0.9524\n",
      " -0.048529   0.24545    0.38401    0.0022835  1.191     -0.30374\n",
      " -0.44441    0.082608   0.23851    0.33268    0.1327     0.38742\n",
      "  0.54447    0.21171   -0.083061   0.2974    -1.0858    -0.36229\n",
      "  0.43826    0.30009   -0.074377  -0.38753   -0.22611    0.071517\n",
      "  0.13473   -0.0738    -0.094661   0.13413    0.21436    0.0081264\n",
      "  0.046816   0.049615   0.98244    0.64103   -0.0082674  0.68412\n",
      " -0.080717   0.15725   -5.6624    -0.60854   -0.60499   -0.31969\n",
      "  0.042107  -0.97399   -0.22257   -0.046089   0.053504   0.14328\n",
      "  0.32858    0.18924   -0.034724   0.04656   -0.15097    0.30726\n",
      "  0.11143   -0.55127   -0.62121    0.717      0.0066723  0.070774\n",
      " -0.15629    0.37838   -0.17075   -0.1874     0.016917   0.18845\n",
      " -1.1152    -0.15614   -0.10915   -0.20781    0.070383  -0.096904\n",
      "  0.55417    0.47062   -0.44501    0.13283   -0.1898    -0.48601\n",
      "  0.79041   -0.13132    1.0374     0.14877   -1.1636    -0.53053\n",
      "  0.1889     0.12433   -0.39479   -0.70221   -2.0926     0.54484\n",
      " -0.19711    0.18826    0.95606   -0.84976    0.054376   0.014243\n",
      " -0.70101   -0.0082734 -0.28095    0.58022    0.19792   -0.6023\n",
      " -0.070511  -0.13207    0.37856   -1.1142     0.20955    0.47169\n",
      " -0.74701   -0.71499   -0.6417     0.12979    0.77034    0.060835\n",
      " -0.39208    1.4978    -1.1134    -0.59952   -1.2542    -0.65079\n",
      "  1.4982     0.40878    0.87208    0.23229   -0.24253   -0.65425\n",
      "  0.45973   -0.16703   -1.2028     0.41675    0.14643   -0.39861\n",
      " -0.35118   -0.46944    0.63799    0.49569   -0.038122  -0.37854\n",
      " -1.2221    -1.0439    -1.2604     0.01232   -0.5159     0.1357\n",
      " -0.093283   0.12307    0.48072   -0.66419    0.50046   -0.58255\n",
      "  0.81583    0.72197   -0.101     -0.17283    0.51572    0.3296\n",
      " -0.0024615  0.19475    2.1163     0.20636   -1.2026    -0.0767\n",
      " -0.1058    -0.82518   -0.31287   -0.19303    0.061489  -0.30422\n",
      "  0.75731   -0.53688   -0.22277   -0.22173   -0.37943    0.17821\n",
      " -0.34743    0.3064     0.45973   -0.16703   -1.2028     0.41675\n",
      "  0.14643   -0.39861   -0.35118   -0.46944    0.63799    0.49569\n",
      " -0.038122  -0.37854   -1.2221    -1.0439    -1.2604     0.01232\n",
      " -0.5159     0.1357    -0.093283   0.12307    0.48072   -0.66419\n",
      "  0.50046   -0.58255    0.81583    0.72197   -0.101     -0.17283\n",
      "  0.51572    0.3296    -0.0024615  0.19475    2.1163     0.20636\n",
      " -1.2026    -0.0767    -0.1058    -0.82518   -0.31287   -0.19303\n",
      "  0.061489  -0.30422    0.75731   -0.53688   -0.22277   -0.22173\n",
      " -0.37943    0.17821   -0.34743    0.3064   ]\n",
      "shape of tweet vector: (250,)\n"
     ]
    }
   ],
   "source": [
    "# same thing, but with the function\n",
    "np.set_printoptions(suppress=True)\n",
    "for tweet in test_tweets:\n",
    "    tweet_vec = pt.vectorize_tweet(tweet, dict_glove_embs, 5)\n",
    "    print(f\"{tweet}: {tweet_vec}\")\n",
    "    print(f\"shape of tweet vector: {tweet_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462cf3b-92c5-4020-9479-ee6adb272481",
   "metadata": {},
   "source": [
    "## Vectorize tweet data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "214215fc-1f46-4e8d-a3a5-42c805e77b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_clean_v09 = pd.read_csv(\"./data/train_clean_v09.csv\", encoding=\"utf8\")\n",
    "train_clean_vec_id = df_train_clean_v09['id']\n",
    "train_clean_vec_text = df_train_clean_v09['text']\n",
    "train_clean_vec_target = df_train_clean_v09['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a5988e0-bee5-4a4f-ba62-ed40c6a9742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test by tokenizing first 5 tokens only\n",
    "# train_clean_vec_text_vector = [pt.vectorize_tweet(tweet, dict_glove_embs, 5) for tweet in train_clean_vec_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "271f61fe-4712-45d3-8d21-da2d669d324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_clean_vec_text_vector))      # 7485\n",
    "# print(type(train_clean_vec_text_vector[0]))  # should be <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108cd459-9950-405b-a151-8acbc9f5eb34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# text_vector_list = [random.choices(range(100), k=1000) for _ in range(5)]\n",
    "# print(len(text_vector_list))\n",
    "# # text_vector_list[0]  # 1000 ints between 0 and 100\n",
    "# train_clean_vec_text_vector_alt = [random.choices(range(100),k=1000) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "123885e0-6e00-48e0-b696-3025bced698d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check_index = 26  # \"be nyc last week\"\n",
    "# print(f\"embedding vector count: {len(train_clean_vec_text_vector)}\")\n",
    "# print(f\"for tweet: {train_clean_vec_text[check_index]} | vector: {train_clean_vec_text_vector[check_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6cba6fb-1453-4558-a6a4-0a7432585a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize train padded to 30 tokens\n",
    "train_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in train_clean_vec_text]\n",
    "# only need the version below to properly write to csv without truncation\n",
    "# train_pad30_list = [list(pt.vectorize_tweet(tweet, dict_glove_embs)) for tweet in train_clean_vec_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "207a502f-aa57-4246-a8f0-c7a807237935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(train_clean_vec_text_vector_pad30[0]))        # <class 'numpy.ndarray'>\n",
    "# print(type(train_clean_vec_text_vector_pad30[0].dtype))  # <class 'numpy.dtypes.Float64DType'>\n",
    "# print(type(train_clean_vec_text_vector_pad30))     # list of numpy arrays\n",
    "# print(type(train_clean_vec_text_vector_pad30[0]))  # <class 'numpy.ndarray'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c01864fd-367e-4a61-b599-64888f9116cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create dataframe of vectorize original training data\n",
    "# df_train_clean_vec = pd.DataFrame({\"id\": train_clean_vec_id,\n",
    "#                                    \"text\": train_clean_vec_text,\n",
    "#                                    # \"text_vector\": train_clean_vec_text_vector_pad30,\n",
    "#                                    \"text_vector\": train_pad30_list,\n",
    "#                                    \"target\": train_clean_vec_target})\n",
    "# df_train_clean_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc7cfe5-d702-4ceb-b818-8d349b99355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write cleaned and vectorize original training data\n",
    "# df_train_clean_vec.to_csv(path_or_buf=\"./data/train_clean_vec.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ceb30b39-795f-4711-800c-c3e651a10522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 1500)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feat_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m feat_matrix_train \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mmake_tweet_feats(train_clean_vec_text_vector_pad30)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(feat_matrix_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfeat_matrix\u001b[49m[:\u001b[38;5;241m3\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feat_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "feat_matrix_train = pt.make_tweet_feats(train_clean_vec_text_vector_pad30)\n",
    "print(feat_matrix_train.shape, feat_matrix_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59062d86-fe6a-468b-9b2d-e40159202fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8568   ,  0.76506  , -0.0091393],\n",
       "       [-0.18449  , -0.93946  , -0.16102  ],\n",
       "       [ 0.33808  ,  0.24919  ,  0.25473  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_matrix_train[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a976856a-2f6a-4f90-8b21-3754735caf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname='./data/feat_matrix_train.txt', X=feat_matrix_train, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1853c789-cef0-4934-9301-de329b898294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 1500) float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.8568   ,  0.76506  , -0.0091393],\n",
       "       [-0.18449  , -0.93946  , -0.16102  ],\n",
       "       [ 0.33808  ,  0.24919  ,  0.25473  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test retrieval\n",
    "x = np.loadtxt(fname='./data/feat_matrix_train.txt')\n",
    "print(x.shape, x.dtype)\n",
    "feat_matrix_train[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc4b6eed-2992-49b1-ba99-093817383f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix for augmented data\n",
    "df_aug_clean_v09 = pd.read_csv(\"./data/aug_clean_v09.csv\", encoding=\"utf8\")\n",
    "aug_clean_vec_id = df_aug_clean_v09['id']\n",
    "aug_clean_vec_text = df_aug_clean_v09['text']\n",
    "aug_clean_vec_target = df_aug_clean_v09['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5eac4bcb-fe7e-430c-88aa-5508efe0efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize aug padded to 30 tokens\n",
    "aug_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in aug_clean_vec_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3861b050-0cd9-4ca8-b549-1f311142c81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485, 1500) float64\n"
     ]
    }
   ],
   "source": [
    "# print(len(aug_clean_vec_text_vector_pad30), aug_clean_vec_text_vector_pad30[0].shape)  # (7485, (1500,)) check\n",
    "feat_matrix_aug = pt.make_tweet_feats(aug_clean_vec_text_vector_pad30)  # takes ~1min, 20sec to run\n",
    "print(feat_matrix_aug.shape, feat_matrix_aug.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5855bb55-8174-420b-ada8-1ae0454cdcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname='./data/feat_matrix_aug.txt', X=feat_matrix_aug, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c4b183e-940b-4c01-bf1e-8ad0d4b165b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix for test data\n",
    "df_test_clean_v09 = pd.read_csv(\"./data/test_clean_v09.csv\", encoding=\"utf8\")\n",
    "test_clean_vec_id = df_test_clean_v09['id']\n",
    "test_clean_vec_text = df_test_clean_v09['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e1a6c83-060e-4af2-99eb-981375ea759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize test padded to 30 tokens\n",
    "test_clean_vec_text_vector_pad30 = [pt.vectorize_tweet(tweet, dict_glove_embs) for tweet in test_clean_vec_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1c895e9-974b-489f-863b-8756c0db9a9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 1500) float64\n"
     ]
    }
   ],
   "source": [
    "print(len(test_clean_vec_text_vector_pad30), test_clean_vec_text_vector_pad30[0].shape)  # (3263, (1500,)) check\n",
    "feat_matrix_test = pt.make_tweet_feats(test_clean_vec_text_vector_pad30)  # takes ~1min, 20sec to run\n",
    "print(feat_matrix_test.shape, feat_matrix_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c0876e9-4843-4757-89c0-299883db8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname='./data/feat_matrix_test.txt', X=feat_matrix_test, fmt='%9.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a1a23d7-0f71-494c-a502-4e16e333061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the train and aug targets - these should be identical - diff confirmed\n",
    "# np.savetxt(fname='./data/targets_train.txt', X=train_clean_vec_target, fmt='%1d')\n",
    "# np.savetxt(fname='./data/targets_aug.txt', X=aug_clean_vec_target, fmt='%1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4f68dea-e0b8-41c8-9f90-40942270a210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets = np.array(train_clean_vec_target)\n",
    "print(train_targets.shape)\n",
    "type(train_targets)\n",
    "train_targets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90799600-c56a-4138-be3d-68b38b9ad4e3",
   "metadata": {},
   "source": [
    "## Logistic regression models\n",
    "\n",
    "### Just the original training data\n",
    "\n",
    "**Results look pretty good:**\n",
    "\n",
    "+ Train Training error:  0.201737\n",
    "+ Train Test error:  0.275217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cced625-1713-465d-8055-e2abaf296bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7485 total train samples, 5988 train_train samples, 1497 train_test samples\n",
      "748 class 0 train test samples, 749 class 1 train test samples\n"
     ]
    }
   ],
   "source": [
    "# split training into train and train_test because test data is unlabeled\n",
    "np.random.seed(711)\n",
    "portion_test = 0.20  # 80/20 train/test split\n",
    "train_train_samples = int((1. - portion_test) * train_targets.shape[0])\n",
    "train_test_samples = train_targets.shape[0] - train_train_samples\n",
    "portion_class0_test = 0.5\n",
    "class0_test_samples = int(train_test_samples * portion_class0_test)\n",
    "class1_test_samples = train_test_samples - class0_test_samples\n",
    "print(f\"{train_targets.shape[0]} total train samples, {train_train_samples} train_train samples, {train_test_samples} train_test samples\")\n",
    "print(f\"{class0_test_samples} class 0 train test samples, {class1_test_samples} class 1 train test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a1b5c369-caf4-4c60-8a0d-1e260b156744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train TRAIN data feature matrix: (5988, 1500)\n",
      "train TEST data feature matrix: (1497, 1500)\n"
     ]
    }
   ],
   "source": [
    "# indices of 748 class 0 and 749 class 1 random samples, need the [0] on the np.where calls because\n",
    "# it returns a 2-tuple and the results of the 1st logical condition are all we want\n",
    "train_test_inds = np.append(np.random.choice((np.where(train_targets==0))[0], class0_test_samples, replace=False),\n",
    "                            np.random.choice((np.where(train_targets==1))[0], class1_test_samples, replace=False))\n",
    "# build training set from indices not in the set\n",
    "train_train_inds = list(set(range(len(train_targets))) - set(train_test_inds))\n",
    "# original training data used to train model\n",
    "train_train_data = feat_matrix_train[train_train_inds, ]\n",
    "train_train_labels = train_targets[train_train_inds, ]\n",
    "# original training data used to TEST the model (because provided test data is unlabeled)\n",
    "train_test_data = feat_matrix_train[train_test_inds, ]\n",
    "train_test_labels = train_targets[train_test_inds, ]\n",
    "\n",
    "print(f\"train TRAIN data feature matrix: {train_train_data.shape}\")\n",
    "print(f\"train TEST data feature matrix: {train_test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f98571b2-971b-43a7-9044-8d040d56597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "logreg_model_orig = SGDClassifier(loss=\"log_loss\", penalty=None)\n",
    "logreg_model_orig.fit(train_train_data, train_train_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = logreg_model_orig.coef_[0,:]\n",
    "b = logreg_model_orig.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0ac13ec8-668e-4c6a-bbf5-95fbe781fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  0.2017368069472278\n",
      "Test error:  0.27521710086840345\n"
     ]
    }
   ],
   "source": [
    "## Get predictions on training and test data\n",
    "preds_train = logreg_model_orig.predict(train_train_data)\n",
    "preds_test = logreg_model_orig.predict(train_test_data)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (train_test_labels > 0.0))\n",
    "\n",
    "print(\"Training error: \", float(errs_train)/len(train_train_labels))\n",
    "print(\"Test error: \", float(errs_test)/len(train_test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9d5e1-824d-4dec-a018-9d6b02b1a10c",
   "metadata": {},
   "source": [
    "### The original training data PLUS the augmented data\n",
    "\n",
    "Now we'll add the augmented data to the training data and see if this improves our results.\n",
    "\n",
    "**Training error was cut in half, but test only improved slightly (~1.5%):**\n",
    "\n",
    "+ Training error:  0.108365\n",
    "+ Test error:  0.271209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7c7f049-faac-4431-b933-a72b4465f8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7485,)\n",
      "(5988,)\n",
      "(13473, 1500) (13473,)\n"
     ]
    }
   ],
   "source": [
    "# create train + aug feature and label inputs\n",
    "train_train_aug_data = np.vstack((train_train_data, feat_matrix_aug))\n",
    "train_train_aug_labels = np.hstack((train_train_labels, np.array(aug_clean_vec_target)))\n",
    "print(train_train_aug_data.shape, train_train_aug_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a53722a-0020-47f2-8fdb-4b309c897fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit logistic classifier on training data: minimize neg log likelihood (\"log\"), no regularization penalty\n",
    "logreg_model_aug = SGDClassifier(loss=\"log_loss\", penalty=None)\n",
    "logreg_model_aug.fit(train_train_aug_data, train_train_aug_labels)\n",
    "\n",
    "## Pull out the parameters (w,b) of the logistic regression model\n",
    "w = logreg_model_aug.coef_[0,:]\n",
    "b = logreg_model_aug.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7fc9a1f2-983c-49fb-86b3-5b9445770baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error with aug data:  0.10836487790395606\n",
      "Test error with aug data:  0.2712090848363393\n"
     ]
    }
   ],
   "source": [
    "## Get predictions on training and test data\n",
    "preds_train_aug = logreg_model_aug.predict(train_train_aug_data)\n",
    "preds_test_aug = logreg_model_aug.predict(train_test_data)  # same train TEST partition, but different model\n",
    "\n",
    "## Compute errors\n",
    "errs_train_aug = np.sum((preds_train_aug > 0.0) != (train_train_aug_labels > 0.0))\n",
    "errs_test_aug = np.sum((preds_test_aug > 0.0) != (train_test_labels > 0.0))  # same train TEST partition, but different model\n",
    "\n",
    "print(\"Training error with aug data: \", float(errs_train_aug)/len(train_train_aug_labels))\n",
    "print(\"Test error with aug data: \", float(errs_test_aug)/len(train_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51964ba7-0960-4caf-a97b-bb84eda7d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c8e75e-7826-4df7-b12c-c1070fa7c7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd1f579-2c09-4564-b24c-d67c10480c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff714859-ddbd-410c-85fc-de047551abe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb91b27-26e9-46d9-8079-689e9d238a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe11bb5-6846-46da-a716-eeea470742ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649a5f6-db24-439f-a5c6-be4329d6e5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696941-a04d-4f3f-aeec-e133153a1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36480-1825-4f2a-8237-a9b1a73ff208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42422a-cfdf-4886-8076-ca74845a23f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025200d-3d63-402c-9545-7d93cbc76e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b72f0-3763-4933-962a-89048aa38d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7641e-4723-4cf3-b5a3-87377bc38c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
