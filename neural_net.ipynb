{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fc89f8-52f3-40d3-98d4-ce3938056e91",
   "metadata": {},
   "source": [
    "# LLM Training Data Augmentation - Classification of Kaggle Disaster Data\n",
    "\n",
    "The goal of this notebook is to prepare the data for augmentation by an LLM and classification by two models:\n",
    "\n",
    "1. Logistic regression\n",
    "2. Single hidden-layer neural network\n",
    "\n",
    "## Data\n",
    "\n",
    "The data used in this project comes from the kaggle *Natural Language Processing with Disaster Tweets* competition at:  \n",
    "\n",
    "https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "This data consists of two files:\n",
    "+ *train.csv* - 7485 labled tweets **after duplicate removals** \n",
    "+ *test.csv* - 3263 unlabled tweets\n",
    "\n",
    "Because the *test.csv* labels are not available, the *train.csv* file was split into the following two files:\n",
    "\n",
    "+ train_model.csv - data used to train model, 5988 labeled tweets\n",
    "+ train_test.csv - held out and not used to train model, used as *pseudo-test* data, 1497 labeled tweets (~20% of the original training sample)\n",
    "\n",
    "## Simplier NLP Classifier Models\n",
    "\n",
    "Two types of models are created and compared:\n",
    "\n",
    "1. Logistic Regression - This serves as the baseline\n",
    "2. Single-Hidden layer neural network with 1000 nodes in the hidden layer\n",
    "\n",
    "## LLM\n",
    "\n",
    "ChatGPT 3.5 turbo will be used to augment the data used to train the models.\n",
    "\n",
    "## Encodings\n",
    "\n",
    "The Twitter GloVe embedding will be used to vectorize the input text.  These embeddings were downloaded from:\n",
    "\n",
    "https://nlp.stanford.edu/data/glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ab98c-934b-4a3c-8a87-a97cce7aa76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50491f7f-8981-4e91-9eba-f7f42d4c9795",
   "metadata": {},
   "source": [
    "## Vectorize with all cleaned tweet tokens\n",
    "\n",
    "Since the max number of tokens in the cleaned original training data is 29 and 26 for the cleaned augmented data, a 30 token input will be selected.  This will give us an input to the model that is 30 (tokens / tweet) x (50 dimensions / token) = 1500 dimensions / tweet.\n",
    "\n",
    "Since all tweets will be less than 30 tokens, each input will be padded with the empty string token (<>).\n",
    "\n",
    "## Build feature matrices\n",
    "\n",
    "The following 4 feature matrices are built and exported so the can be read back in during modeling:\n",
    "\n",
    "+ **feats_matrix_aug.txt** - 7485 rows where each row is a vectorized tweet padded to 30 tokens where each token is represented by a 50d GloVe twitter embedding and the empty string is used as the padding token.  1500 cols are the tweets padded to 30 tokens which are each converted to a 50d GloVe embedding\n",
    "+ **feats_matrix_train_train.txt** - 80% of the original training data used to train each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_train_test.txt** - 20% of the original training data used to test each model, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "+ **feats_matrix_test.txt** - unlabeled test data provided by kaggle to test submissions, same vectorization as **feats_matrix_aug.txt**, xxxx rows, yyyy columns\n",
    "\n",
    "The first 3 feature matrices have the following corresponding labels (`feats_matrix_test.txt` are unlabeled tweets):\n",
    "+ labels_aug.txt\n",
    "+ labels_train_train.txt\n",
    "+ labels_train_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8af07166-b4a7-4a1c-8d95-ef94f121a9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\llmamd'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import projtools as pt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c34184-d8aa-44c6-be99-e0a2b99a99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train train feature matrix: (5988, 1500), shape of train train labels: (5988,)\n",
      "dtype of train train labels: int32\n",
      "shape of train test feature matrix: (1497, 1500), shape of train test labels: (1497,)\n",
      "dtype of train test labels: int32\n",
      "shape of augmented feature matrix: (7485, 1500), shape of augmented labels: (7485,)\n",
      "dtype of augmented labels: int32\n",
      "shape of (unlabeled) test feature matrix: (3263, 1500)\n"
     ]
    }
   ],
   "source": [
    "# read in the feature matrices\n",
    "feats_matrix_train_train = np.loadtxt('./data/feats_matrix_train_train.txt')\n",
    "feats_matrix_train_test = np.loadtxt('./data/feats_matrix_train_test.txt')\n",
    "feats_matrix_aug = np.loadtxt('./data/feats_matrix_aug.txt')\n",
    "feats_matrix_test = np.loadtxt('./data/feats_matrix_test.txt')\n",
    "# read in the labels\n",
    "labels_train_train = np.loadtxt('./data/labels_train_train.txt', dtype='int')\n",
    "labels_train_test = np.loadtxt('./data/labels_train_test.txt', dtype='int')\n",
    "labels_aug = np.loadtxt('./data/labels_aug.txt', dtype='int')\n",
    "print(f\"shape of train train feature matrix: {feats_matrix_train_train.shape}, shape of train train labels: {labels_train_train.shape}\")\n",
    "print(f\"dtype of train train labels: {labels_train_train.dtype}\")\n",
    "print(f\"shape of train test feature matrix: {feats_matrix_train_test.shape}, shape of train test labels: {labels_train_test.shape}\")\n",
    "print(f\"dtype of train test labels: {labels_train_test.dtype}\")\n",
    "print(f\"shape of augmented feature matrix: {feats_matrix_aug.shape}, shape of augmented labels: {labels_aug.shape}\")\n",
    "print(f\"dtype of augmented labels: {labels_aug.dtype}\")\n",
    "print(f\"shape of (unlabeled) test feature matrix: {feats_matrix_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a9aae5-acad-46d9-8ed3-79d9946e97a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_train[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90799600-c56a-4138-be3d-68b38b9ad4e3",
   "metadata": {},
   "source": [
    "## Neural network models\n",
    "\n",
    "### Activation function\n",
    "\n",
    "Will experiment with sigmoid and ReLu.\n",
    "\n",
    "### Hidden units\n",
    "\n",
    "Will experiment with 100 and 300 nodes in the single hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8c6beb-5d99-4d6f-9e23-50bdf42ac58f",
   "metadata": {},
   "source": [
    "## 10 Fold CV - ReLU vs. Sigmoid hidden layer activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb91b27-26e9-46d9-8079-689e9d238a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both models have 100 units in their hidden layers with 150201 total parameters (weights and biases)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# compare hidden layer activation functions\n",
    "num_inputs = 1500\n",
    "num_outputs = 1\n",
    "hidden_units = [100, 300]  # number of units in to single hidden layer\n",
    "\n",
    "i = 0  # evaluate activation function on the smaller hidden unit count to speed training\n",
    "# use relu for hidden layer\n",
    "model_h_relu = nn.Sequential(\n",
    "    nn.Linear(num_inputs, hidden_units[i]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_units[i], num_outputs),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# use sigmoid for hidden layer\n",
    "model_h_sigmoid = nn.Sequential(\n",
    "    nn.Linear(num_inputs, hidden_units[i]),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(hidden_units[i], num_outputs),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# both models have the same number of parameters\n",
    "total_params = 0\n",
    "for parameter in model_h_relu.parameters():\n",
    "    total_params += parameter.numel()\n",
    "print(f\"both models have {hidden_units[i]} units in their hidden layers with {total_params} total parameters (weights and biases)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32a639f-e5ab-4ab9-aa8d-bcb5d8c64b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 10-fold CV at Fri Apr 12 16:20:35 2024\n",
      "feature tensor X has shape torch.Size([5988, 1500]), labels tensor y has shape torch.Size([5988, 1])\n",
      "ReLU accuracy: 0.7786\n",
      "ReLU accuracy: 0.7971\n",
      "ReLU accuracy: 0.8186\n",
      "ReLU accuracy: 0.8115\n",
      "ReLU accuracy: 0.8568\n",
      "ReLU accuracy: 0.8282\n",
      "ReLU accuracy: 0.8162\n",
      "ReLU accuracy: 0.8783\n",
      "ReLU accuracy: 0.8616\n",
      "ReLU accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "# compare these 2 models with 10-fold CV\n",
    "import time\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "cv_folds = 10\n",
    "t0 = time.time()  # init start time\n",
    "print(f\"starting {cv_folds}-fold CV at {time.ctime()}\")\n",
    "\n",
    "X = torch.tensor(feats_matrix_train_train, dtype=torch.float32)\n",
    "y = torch.tensor(labels_train_train, dtype=torch.float32).reshape(-1, 1)  # make single col vector (N x 1)\n",
    "print(f\"feature tensor X has shape {X.shape}, labels tensor y has shape {y.shape}\")\n",
    "\n",
    "# train-test split: hold out the test set for final model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "# define 5-fold cv test harness\n",
    "kfold = StratifiedKFold(n_splits=cv_folds, shuffle=True)\n",
    "\n",
    "# evaluate model with ReLU hidden layer activation\n",
    "cv_scores_relu = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = model_h_relu\n",
    "    # train model, grab accuracy from the returned tuple\n",
    "    acc, _ = pt.nn_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(f\"ReLU accuracy: {acc:.4f}\")\n",
    "    cv_scores_relu.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b364118b-7f7d-4d01-8f4f-5cffc0fc5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid accuracy: 0.7762\n",
      "sigmoid accuracy: 0.8067\n",
      "sigmoid accuracy: 0.7971\n",
      "sigmoid accuracy: 0.7661\n",
      "sigmoid accuracy: 0.8401\n",
      "sigmoid accuracy: 0.8091\n",
      "sigmoid accuracy: 0.8544\n",
      "sigmoid accuracy: 0.8377\n",
      "sigmoid accuracy: 0.8282\n",
      "sigmoid accuracy: 0.8377\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with sigmoid hidden layer activation\n",
    "cv_scores_sigmoid = []\n",
    "for train, test in kfold.split(X_train, y_train):\n",
    "    # create model, train, and get accuracy\n",
    "    model = model_h_sigmoid\n",
    "    # train model, grab accuracy from the returned tuple\n",
    "    acc, _ = pt.nn_train(model, X_train[train], y_train[train], X_train[test], y_train[test])\n",
    "    print(f\"sigmoid accuracy: {acc:.4f}\")\n",
    "    cv_scores_sigmoid.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f988d56a-7133-4d9a-9c5a-4274a9dfc13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU accuracy: 83.63% +- 7.877542178298569%\n",
      "sigmoid accuracy: 81.53% +- 5.552229291138861%\n",
      "finishing 10-fold CV at Fri Apr 12 17:00:24 2024\n",
      "CV on two models took 39.81809523105621 minutes to run\n"
     ]
    }
   ],
   "source": [
    "# compare model performance\n",
    "relu_acc = np.mean(cv_scores_relu)\n",
    "relu_std = np.std(cv_scores_relu)\n",
    "sigmoid_acc = np.mean(cv_scores_sigmoid)\n",
    "sigmoid_std = np.std(cv_scores_sigmoid)\n",
    "print(f\"ReLU accuracy: {relu_acc*100:.2f}% +- {2*relu_std*100}%\")\n",
    "print(f\"sigmoid accuracy: {sigmoid_acc*100:.2f}% +- {2*sigmoid_std*100}%\")\n",
    "\n",
    "print(f\"finishing {cv_folds}-fold CV at {time.ctime()}\")\n",
    "t1 = time.time()  # finish cv time\n",
    "exec_time = (t1 - t0) / 60.\n",
    "print(f\"CV on two models took {exec_time} minutes to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415eba1-fd1b-49a9-9e4c-df3005242a2c",
   "metadata": {},
   "source": [
    "## 10 Fold CV - 100 vs. 300 nodes in hidden layer (ReLU activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696941-a04d-4f3f-aeec-e133153a1b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36480-1825-4f2a-8237-a9b1a73ff208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42422a-cfdf-4886-8076-ca74845a23f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025200d-3d63-402c-9545-7d93cbc76e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092512ed-b1d6-45c3-aca8-bb4a5643d9a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9deade-a4fd-43c2-9675-1cad3f3965f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b72f0-3763-4933-962a-89048aa38d83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f525a6-3f50-4de1-8e2e-540cb06ffca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7641e-4723-4cf3-b5a3-87377bc38c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f229bc8d-a043-497c-a637-455a4dbba704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4fa20-b2a4-44c9-9b01-f3208dac8612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaa35c0-d90b-4404-8815-90b6d0ef7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717c837-1f29-41ee-af2b-a4f2c115c7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b743c4f-f6ed-4b70-b83c-1881c7b0e70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffa728-3fe8-4864-a2cd-3be111517140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370df4c-c12b-408f-a0e4-e80bf02ba53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf09c8-2132-48bf-92e4-f67178763e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7616c-9488-4a06-a782-4103d3d9fe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60f35d-b628-4dbe-bd45-f0432d23dce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce33e5b-ffa0-4a32-908a-caf7f9e02be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305007e-7795-486c-bf8a-6ca6968dc9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
